{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of Neural Networks on the Elzermann readout data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-19 13:39:58.671941: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-19 13:39:58.672099: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os, pickle\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Colors using scheme\n",
    "c = [\"#\" + i for i  in \"264653-2a9d8f-e9c46a-f4a261-e76f51\".split(\"-\")]\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "\n",
    "def fft_abs_phase(input):\n",
    "    transformed = np.fft.rfft(input)\n",
    "    output      = np.concatenate([np.abs(transformed), np.cos(np.angle(transformed)), np.sin(np.angle(transformed))],\n",
    "                                  axis = 1)\n",
    "    return output\n",
    "X_train, X_test, y_train, y_test = pd.read_pickle(\"/mnt/c/Users/johan/Desktop/CMT_project/data/train_test_data.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.metrics import AUC, BinaryAccuracy\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras_lr_finder import LRFinder\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs     = 50\n",
    "batch_size = 256\n",
    "timesteps  = 220\n",
    "val_split  = 0.10\n",
    "patience   = 10\n",
    "\n",
    "scaler              = RobustScaler()\n",
    "X_train_scaled      = scaler.fit_transform(X_train)\n",
    "X_test_scaled       = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-19 13:40:05.863908: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-02-19 13:40:05.863961: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-02-19 13:40:05.863980: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-B87RRKQ): /proc/driver/nvidia/version does not exist\n",
      "2022-02-19 13:40:05.864220: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 220)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                14144     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,689\n",
      "Trainable params: 26,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "225/225 [==============================] - 2s 4ms/step - loss: 0.5522 - Acc: 0.7410 - AUC: 0.8065 - val_loss: 0.4902 - val_Acc: 0.7912 - val_AUC: 0.8653\n",
      "Epoch 2/50\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 0.4887 - Acc: 0.7837 - AUC: 0.8559 - val_loss: 0.4841 - val_Acc: 0.8056 - val_AUC: 0.8710\n",
      "Epoch 3/50\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 0.4750 - Acc: 0.7897 - AUC: 0.8623 - val_loss: 0.4649 - val_Acc: 0.8056 - val_AUC: 0.8761\n",
      "Epoch 4/50\n",
      "225/225 [==============================] - 1s 2ms/step - loss: 0.4637 - Acc: 0.7965 - AUC: 0.8688 - val_loss: 0.4739 - val_Acc: 0.7969 - val_AUC: 0.8723\n",
      "Epoch 5/50\n",
      "225/225 [==============================] - 1s 2ms/step - loss: 0.4562 - Acc: 0.7960 - AUC: 0.8716 - val_loss: 0.4635 - val_Acc: 0.7981 - val_AUC: 0.8725\n",
      "Epoch 6/50\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 0.4535 - Acc: 0.7958 - AUC: 0.8714 - val_loss: 0.4699 - val_Acc: 0.7950 - val_AUC: 0.8701\n",
      "Epoch 7/50\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 0.4453 - Acc: 0.8034 - AUC: 0.8784 - val_loss: 0.4640 - val_Acc: 0.8037 - val_AUC: 0.8715\n",
      "Epoch 8/50\n",
      "225/225 [==============================] - 1s 2ms/step - loss: 0.4530 - Acc: 0.8004 - AUC: 0.8752 - val_loss: 0.4696 - val_Acc: 0.7987 - val_AUC: 0.8703\n",
      "Epoch 9/50\n",
      "225/225 [==============================] - 1s 2ms/step - loss: 0.4457 - Acc: 0.8047 - AUC: 0.8773 - val_loss: 0.4827 - val_Acc: 0.8062 - val_AUC: 0.8697\n",
      "Epoch 10/50\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 0.4376 - Acc: 0.8092 - AUC: 0.8816 - val_loss: 0.4593 - val_Acc: 0.8087 - val_AUC: 0.8729\n",
      "Epoch 11/50\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 0.4360 - Acc: 0.8104 - AUC: 0.8822 - val_loss: 0.4637 - val_Acc: 0.8031 - val_AUC: 0.8702\n",
      "Epoch 12/50\n",
      "225/225 [==============================] - 1s 2ms/step - loss: 0.4352 - Acc: 0.8118 - AUC: 0.8825 - val_loss: 0.4546 - val_Acc: 0.8075 - val_AUC: 0.8740\n",
      "Epoch 13/50\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 0.4289 - Acc: 0.8160 - AUC: 0.8857 - val_loss: 0.4580 - val_Acc: 0.8094 - val_AUC: 0.8736\n",
      "Epoch 14/50\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 0.4273 - Acc: 0.8147 - AUC: 0.8868 - val_loss: 0.4570 - val_Acc: 0.8119 - val_AUC: 0.8718\n",
      "Epoch 15/50\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 0.4258 - Acc: 0.8186 - AUC: 0.8876 - val_loss: 0.4509 - val_Acc: 0.8156 - val_AUC: 0.8804\n",
      "Epoch 16/50\n",
      "225/225 [==============================] - 1s 2ms/step - loss: 0.4200 - Acc: 0.8215 - AUC: 0.8902 - val_loss: 0.4553 - val_Acc: 0.7969 - val_AUC: 0.8759\n",
      "Epoch 17/50\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 0.4118 - Acc: 0.8245 - AUC: 0.8939 - val_loss: 0.4549 - val_Acc: 0.8087 - val_AUC: 0.8729\n",
      "Epoch 18/50\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 0.4139 - Acc: 0.8246 - AUC: 0.8942 - val_loss: 0.4765 - val_Acc: 0.8119 - val_AUC: 0.8717\n",
      "Epoch 19/50\n",
      "225/225 [==============================] - 1s 2ms/step - loss: 0.4110 - Acc: 0.8270 - AUC: 0.8951 - val_loss: 0.4561 - val_Acc: 0.7975 - val_AUC: 0.8742\n",
      "Epoch 20/50\n",
      "225/225 [==============================] - 1s 2ms/step - loss: 0.4093 - Acc: 0.8258 - AUC: 0.8955 - val_loss: 0.4604 - val_Acc: 0.8150 - val_AUC: 0.8751\n",
      "Epoch 21/50\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 0.4047 - Acc: 0.8301 - AUC: 0.8982 - val_loss: 0.4579 - val_Acc: 0.8144 - val_AUC: 0.8730\n",
      "Epoch 22/50\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 0.4031 - Acc: 0.8286 - AUC: 0.8999 - val_loss: 0.4588 - val_Acc: 0.8169 - val_AUC: 0.8737\n",
      "Epoch 23/50\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 0.4060 - Acc: 0.8309 - AUC: 0.8980 - val_loss: 0.4658 - val_Acc: 0.8106 - val_AUC: 0.8744\n",
      "Epoch 24/50\n",
      "225/225 [==============================] - 1s 4ms/step - loss: 0.4007 - Acc: 0.8333 - AUC: 0.8995 - val_loss: 0.4559 - val_Acc: 0.8044 - val_AUC: 0.8721\n",
      "Epoch 25/50\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 0.4047 - Acc: 0.8306 - AUC: 0.8990 - val_loss: 0.4577 - val_Acc: 0.8062 - val_AUC: 0.8737\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4578 - Acc: 0.8077 - AUC: 0.8719\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "name           = \"feed_forward_network\"\n",
    "dropout        = 0.25\n",
    "hidden_states  = 64 \n",
    "learning_rate  = 5e-3\n",
    "test_lr        = False\n",
    "test_interval  = (1e-9, 1)\n",
    "fit_model      = True\n",
    "test_model     = True\n",
    "save_preds     = True\n",
    "\n",
    "input = Input(shape = (timesteps))\n",
    "\n",
    "x     = Dense(hidden_states, activation = \"relu\")(input)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "x     = Dense(hidden_states, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "x     = Dense(hidden_states, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "x     = Dense(hidden_states, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "out   = Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs = input, outputs = out)\n",
    "\n",
    "model.compile(\n",
    "    loss    = BinaryCrossentropy(from_logits = False),\n",
    "    metrics   = [BinaryAccuracy(name = \"Acc\"), AUC(name = \"AUC\", from_logits = True)],\n",
    "    optimizer = Adam(learning_rate = learning_rate)\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "if test_lr:\n",
    "    finder = LRFinder(model)\n",
    "    finder.find(X_train_scaled, y_train, test_interval[0], test_interval[1], batch_size = batch_size)\n",
    "    finder.plot_loss()\n",
    "\n",
    "if fit_model:\n",
    "    model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs = epochs,\n",
    "        batch_size = batch_size,\n",
    "        validation_split = val_split,\n",
    "        callbacks = EarlyStopping(\"val_loss\", patience = patience, restore_best_weights = True)\n",
    "        )\n",
    "\n",
    "if test_model:\n",
    "    model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "    test_preds = model(X_test_scaled)\n",
    "\n",
    "    if save_preds:\n",
    "        with open(f\"../predictions/{name}.dat\", \"wb\") as file:\n",
    "            pickle.dump((test_preds, y_test), file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D-conv:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, Flatten, Normalization\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "lr_schedule_func = lambda epoch, lr: lr if epoch < 5 else 0.90 * lr\n",
    "\n",
    "# qnt_scaler = QuantileTransformer()\n",
    "# qnt_scaler.fit(X_train)\n",
    "\n",
    "# norm_layer = Normalization(mean = tf.convert_to_tensor(scaler.center_, dtype = tf.float32), variance = tf.convert_to_tensor(scaler.scale_ ** 2, dtype = tf.float32))\n",
    "norm_layer = Normalization()\n",
    "norm_layer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_28 (InputLayer)       [(None, 220)]             0         \n",
      "                                                                 \n",
      " normalization_16 (Normaliza  (None, 220)              441       \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " tf.expand_dims_27 (TFOpLamb  (None, 220, 1)           0         \n",
      " da)                                                             \n",
      "                                                                 \n",
      " conv1d_102 (Conv1D)         (None, 120, 32)           3264      \n",
      "                                                                 \n",
      " conv1d_103 (Conv1D)         (None, 70, 16)            26128     \n",
      "                                                                 \n",
      " conv1d_104 (Conv1D)         (None, 46, 16)            6416      \n",
      "                                                                 \n",
      " conv1d_105 (Conv1D)         (None, 37, 8)             1288      \n",
      "                                                                 \n",
      " flatten_25 (Flatten)        (None, 296)               0         \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 32)                9504      \n",
      "                                                                 \n",
      " dropout_50 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_76 (Dense)            (None, 8)                 264       \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 47,314\n",
      "Trainable params: 46,873\n",
      "Non-trainable params: 441\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "225/225 [==============================] - 5s 21ms/step - loss: 0.5774 - Acc: 0.6893 - AUC: 0.7844 - val_loss: 0.4525 - val_Acc: 0.8194 - val_AUC: 0.8841 - lr: 5.0000e-04\n",
      "Epoch 2/50\n",
      "225/225 [==============================] - 4s 18ms/step - loss: 0.5142 - Acc: 0.7822 - AUC: 0.8457 - val_loss: 0.4448 - val_Acc: 0.8250 - val_AUC: 0.8899 - lr: 5.0000e-04\n",
      "Epoch 3/50\n",
      "225/225 [==============================] - 4s 20ms/step - loss: 0.4870 - Acc: 0.7962 - AUC: 0.8605 - val_loss: 0.4211 - val_Acc: 0.8281 - val_AUC: 0.8907 - lr: 5.0000e-04\n",
      "Epoch 4/50\n",
      "225/225 [==============================] - 5s 21ms/step - loss: 0.4757 - Acc: 0.8052 - AUC: 0.8682 - val_loss: 0.4179 - val_Acc: 0.8394 - val_AUC: 0.9019 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.4684 - Acc: 0.8109 - AUC: 0.8740 - val_loss: 0.4044 - val_Acc: 0.8363 - val_AUC: 0.9020 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.4628 - Acc: 0.8203 - AUC: 0.8775 - val_loss: 0.4235 - val_Acc: 0.8475 - val_AUC: 0.9041 - lr: 4.5000e-04\n",
      "Epoch 7/50\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.4515 - Acc: 0.8263 - AUC: 0.8829 - val_loss: 0.3952 - val_Acc: 0.8569 - val_AUC: 0.9085 - lr: 4.0500e-04\n",
      "Epoch 8/50\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.4500 - Acc: 0.8289 - AUC: 0.8831 - val_loss: 0.3814 - val_Acc: 0.8550 - val_AUC: 0.9079 - lr: 3.6450e-04\n",
      "Epoch 9/50\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.4418 - Acc: 0.8306 - AUC: 0.8874 - val_loss: 0.3837 - val_Acc: 0.8550 - val_AUC: 0.9072 - lr: 3.2805e-04\n",
      "Epoch 10/50\n",
      "225/225 [==============================] - 5s 21ms/step - loss: 0.4359 - Acc: 0.8317 - AUC: 0.8881 - val_loss: 0.3744 - val_Acc: 0.8562 - val_AUC: 0.9126 - lr: 2.9525e-04\n",
      "Epoch 11/50\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.4358 - Acc: 0.8363 - AUC: 0.8901 - val_loss: 0.3734 - val_Acc: 0.8581 - val_AUC: 0.9126 - lr: 2.6572e-04\n",
      "Epoch 12/50\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.4307 - Acc: 0.8406 - AUC: 0.8937 - val_loss: 0.3721 - val_Acc: 0.8531 - val_AUC: 0.9100 - lr: 2.3915e-04\n",
      "Epoch 13/50\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.4241 - Acc: 0.8412 - AUC: 0.8955 - val_loss: 0.3794 - val_Acc: 0.8556 - val_AUC: 0.9088 - lr: 2.1523e-04\n",
      "Epoch 14/50\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.4232 - Acc: 0.8440 - AUC: 0.8961 - val_loss: 0.3710 - val_Acc: 0.8587 - val_AUC: 0.9112 - lr: 1.9371e-04\n",
      "Epoch 15/50\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.4188 - Acc: 0.8450 - AUC: 0.8991 - val_loss: 0.3706 - val_Acc: 0.8537 - val_AUC: 0.9118 - lr: 1.7434e-04\n",
      "Epoch 16/50\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.4123 - Acc: 0.8480 - AUC: 0.9016 - val_loss: 0.3805 - val_Acc: 0.8512 - val_AUC: 0.9094 - lr: 1.5691e-04\n",
      "Epoch 17/50\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.4120 - Acc: 0.8482 - AUC: 0.9013 - val_loss: 0.3709 - val_Acc: 0.8600 - val_AUC: 0.9115 - lr: 1.4121e-04\n",
      "Epoch 18/50\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.4040 - Acc: 0.8537 - AUC: 0.9042 - val_loss: 0.3774 - val_Acc: 0.8562 - val_AUC: 0.9101 - lr: 1.2709e-04\n",
      "Epoch 19/50\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.4027 - Acc: 0.8539 - AUC: 0.9046 - val_loss: 0.3752 - val_Acc: 0.8569 - val_AUC: 0.9096 - lr: 1.1438e-04\n",
      "Epoch 20/50\n",
      "225/225 [==============================] - 5s 23ms/step - loss: 0.4043 - Acc: 0.8566 - AUC: 0.9047 - val_loss: 0.3800 - val_Acc: 0.8587 - val_AUC: 0.9080 - lr: 1.0295e-04\n",
      "Epoch 21/50\n",
      "225/225 [==============================] - 5s 24ms/step - loss: 0.3993 - Acc: 0.8587 - AUC: 0.9076 - val_loss: 0.3730 - val_Acc: 0.8575 - val_AUC: 0.9100 - lr: 9.2651e-05\n",
      "Epoch 22/50\n",
      "225/225 [==============================] - 5s 23ms/step - loss: 0.3956 - Acc: 0.8615 - AUC: 0.9084 - val_loss: 0.3811 - val_Acc: 0.8519 - val_AUC: 0.9082 - lr: 8.3386e-05\n",
      "Epoch 23/50\n",
      "225/225 [==============================] - 5s 23ms/step - loss: 0.3909 - Acc: 0.8606 - AUC: 0.9099 - val_loss: 0.3751 - val_Acc: 0.8569 - val_AUC: 0.9094 - lr: 7.5047e-05\n",
      "Epoch 24/50\n",
      "225/225 [==============================] - 5s 23ms/step - loss: 0.3941 - Acc: 0.8626 - AUC: 0.9099 - val_loss: 0.3827 - val_Acc: 0.8519 - val_AUC: 0.9077 - lr: 6.7543e-05\n",
      "Epoch 25/50\n",
      "225/225 [==============================] - 5s 23ms/step - loss: 0.3882 - Acc: 0.8655 - AUC: 0.9116 - val_loss: 0.3778 - val_Acc: 0.8531 - val_AUC: 0.9091 - lr: 6.0788e-05\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.3702 - Acc: 0.8525 - AUC: 0.9122\n",
      "INFO:tensorflow:Assets written to: ../models/1D_conv/assets\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "name           = \"1D_conv\"\n",
    "dropout        = 0.5\n",
    "filter_sizes   = (32,  16, 16, 8)\n",
    "kernal_sizes   = (101, 51, 25, 10)\n",
    "hidden_states  = 32\n",
    "learning_rate  = 5e-4\n",
    "test_lr        = False\n",
    "test_interval  = (1e-9, 10)\n",
    "fit_model      = True\n",
    "test_model     = True\n",
    "save_preds     = True\n",
    "save_model     = True\n",
    "\n",
    "\n",
    "input = Input(shape = (timesteps))\n",
    "\n",
    "x = norm_layer(input)\n",
    "\n",
    "x = tf.expand_dims(x, -1)\n",
    "\n",
    "for fs, ks in zip(filter_sizes, kernal_sizes):\n",
    "    x     = Conv1D(fs, kernel_size = ks, activation = \"relu\")(x)\n",
    "\n",
    "x     = Flatten()(x)\n",
    "\n",
    "# x     = tf.squeeze(x, -1)\n",
    "\n",
    "x     = Dense(hidden_states, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "x     = Dense(hidden_states // 4, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "out   = Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs = input, outputs = out)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss    = BinaryCrossentropy(from_logits = False),\n",
    "    metrics   = [BinaryAccuracy(name = \"Acc\"), AUC(name = \"AUC\", from_logits = True)],\n",
    "    optimizer = Adam(learning_rate = learning_rate)\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "if test_lr:\n",
    "    finder = LRFinder(model)\n",
    "    finder.find(X_train_scaled, y_train, test_interval[0], test_interval[1], batch_size = batch_size)\n",
    "    finder.plot_loss()\n",
    "\n",
    "if fit_model:\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs = epochs,\n",
    "        batch_size = batch_size,\n",
    "        validation_split = val_split,\n",
    "        callbacks = [LearningRateScheduler(lr_schedule_func, verbose = 0),\n",
    "                     EarlyStopping(\"val_loss\", patience = patience, restore_best_weights = True)]\n",
    "        )\n",
    "\n",
    "if test_model:\n",
    "    model.evaluate(X_test, y_test)\n",
    "\n",
    "    test_preds = model(X_test)\n",
    "\n",
    "    if save_preds:\n",
    "        with open(f\"../predictions/{name}.dat\", \"wb\") as file:\n",
    "            pickle.dump((test_preds, y_test), file)\n",
    "\n",
    "if save_model:\n",
    "    model.save(f\"../models/{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"../models/{name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Attention and Normalization\n",
    "    x = MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(inputs, inputs)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x + res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_18 (InputLayer)          [(None, 220, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention_18 (Multi  (None, 220, 1)      449         ['input_18[0][0]',               \n",
      " HeadAttention)                                                   'input_18[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_48 (Dropout)           (None, 220, 1)       0           ['multi_head_attention_18[1][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_33 (LayerN  (None, 220, 1)      2           ['dropout_48[1][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_33 (TFOpL  (None, 220, 1)      0           ['layer_normalization_33[1][0]', \n",
      " ambda)                                                           'input_18[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_42 (Conv1D)             (None, 220, 32)      64          ['tf.__operators__.add_33[1][0]']\n",
      "                                                                                                  \n",
      " dropout_49 (Dropout)           (None, 220, 32)      0           ['conv1d_42[1][0]']              \n",
      "                                                                                                  \n",
      " conv1d_43 (Conv1D)             (None, 220, 1)       33          ['dropout_49[1][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_34 (LayerN  (None, 220, 1)      2           ['conv1d_43[1][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_34 (TFOpL  (None, 220, 1)      0           ['layer_normalization_34[1][0]', \n",
      " ambda)                                                           'tf.__operators__.add_33[1][0]']\n",
      "                                                                                                  \n",
      " multi_head_attention_19 (Multi  (None, 220, 1)      449         ['tf.__operators__.add_34[1][0]',\n",
      " HeadAttention)                                                   'tf.__operators__.add_34[1][0]']\n",
      "                                                                                                  \n",
      " dropout_50 (Dropout)           (None, 220, 1)       0           ['multi_head_attention_19[1][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_35 (LayerN  (None, 220, 1)      2           ['dropout_50[1][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_35 (TFOpL  (None, 220, 1)      0           ['layer_normalization_35[1][0]', \n",
      " ambda)                                                           'tf.__operators__.add_34[1][0]']\n",
      "                                                                                                  \n",
      " conv1d_44 (Conv1D)             (None, 220, 32)      64          ['tf.__operators__.add_35[1][0]']\n",
      "                                                                                                  \n",
      " dropout_51 (Dropout)           (None, 220, 32)      0           ['conv1d_44[1][0]']              \n",
      "                                                                                                  \n",
      " conv1d_45 (Conv1D)             (None, 220, 1)       33          ['dropout_51[1][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_36 (LayerN  (None, 220, 1)      2           ['conv1d_45[1][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_36 (TFOpL  (None, 220, 1)      0           ['layer_normalization_36[1][0]', \n",
      " ambda)                                                           'tf.__operators__.add_35[1][0]']\n",
      "                                                                                                  \n",
      " multi_head_attention_20 (Multi  (None, 220, 1)      449         ['tf.__operators__.add_36[1][0]',\n",
      " HeadAttention)                                                   'tf.__operators__.add_36[1][0]']\n",
      "                                                                                                  \n",
      " dropout_52 (Dropout)           (None, 220, 1)       0           ['multi_head_attention_20[1][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_37 (LayerN  (None, 220, 1)      2           ['dropout_52[1][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_37 (TFOpL  (None, 220, 1)      0           ['layer_normalization_37[1][0]', \n",
      " ambda)                                                           'tf.__operators__.add_36[1][0]']\n",
      "                                                                                                  \n",
      " conv1d_46 (Conv1D)             (None, 220, 32)      64          ['tf.__operators__.add_37[1][0]']\n",
      "                                                                                                  \n",
      " dropout_53 (Dropout)           (None, 220, 32)      0           ['conv1d_46[1][0]']              \n",
      "                                                                                                  \n",
      " conv1d_47 (Conv1D)             (None, 220, 1)       33          ['dropout_53[1][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_38 (LayerN  (None, 220, 1)      2           ['conv1d_47[1][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_38 (TFOpL  (None, 220, 1)      0           ['layer_normalization_38[1][0]', \n",
      " ambda)                                                           'tf.__operators__.add_37[1][0]']\n",
      "                                                                                                  \n",
      " flatten_7 (Flatten)            (None, 220)          0           ['tf.__operators__.add_38[1][0]']\n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 32)           7072        ['flatten_7[1][0]']              \n",
      "                                                                                                  \n",
      " dropout_54 (Dropout)           (None, 32)           0           ['dense_26[1][0]']               \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 8)            264         ['dropout_54[1][0]']             \n",
      "                                                                                                  \n",
      " dropout_55 (Dropout)           (None, 8)            0           ['dense_27[1][0]']               \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 1)            9           ['dropout_55[1][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,995\n",
      "Trainable params: 8,995\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      " 3/57 [>.............................] - ETA: 6:50 - loss: 0.6909 - Acc: 0.5130 - AUC: 0.5236"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_335/3087172185.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     model.fit(\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3128\u001b[0m       (graph_function,\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "name           = \"Attention\"\n",
    "dropout        = 0.25\n",
    "# filter_sizes   = (32,  16, 16, 8)\n",
    "# kernal_sizes   = (101, 51, 25, 10)\n",
    "attn_heads     = 8\n",
    "key_dim        = 8\n",
    "\n",
    "\n",
    "hidden_states  = 32\n",
    "learning_rate  = 1e-2\n",
    "test_lr        = False\n",
    "test_interval  = (1e-9, 10)\n",
    "fit_model      = True\n",
    "test_model     = True\n",
    "save_preds     = False\n",
    "save_model     = False\n",
    "\n",
    "\n",
    "input = Input(shape = (timesteps))\n",
    "input = norm_layer(input)\n",
    "input = tf.expand_dims(input, -1)\n",
    "x     = input\n",
    "\n",
    "for _ in range(3):\n",
    "    x = transformer_block(x, head_size = key_dim, num_heads = attn_heads, ff_dim = hidden_states, dropout = dropout)\n",
    "\n",
    "x     = Flatten()(x)\n",
    "\n",
    "x     = Dense(hidden_states, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "x     = Dense(hidden_states // 4, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "out   = Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs = input, outputs = out)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss    = BinaryCrossentropy(from_logits = False),\n",
    "    metrics   = [BinaryAccuracy(name = \"Acc\"), AUC(name = \"AUC\", from_logits = True)],\n",
    "    optimizer = Adam(learning_rate = learning_rate)\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "if test_lr:\n",
    "    finder = LRFinder(model)\n",
    "    finder.find(X_train_scaled, y_train, test_interval[0], test_interval[1], batch_size = batch_size)\n",
    "    finder.plot_loss()\n",
    "\n",
    "if fit_model:\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs = epochs,\n",
    "        batch_size = batch_size,\n",
    "        validation_split = val_split,\n",
    "        callbacks = [LearningRateScheduler(lr_schedule_func, verbose = 0),\n",
    "                     EarlyStopping(\"val_loss\", patience = patience, restore_best_weights = True)]\n",
    "        )\n",
    "\n",
    "if test_model:\n",
    "    model.evaluate(X_test, y_test)\n",
    "\n",
    "    test_preds = model(X_test)\n",
    "\n",
    "    if save_preds:\n",
    "        with open(f\"../predictions/{name}.dat\", \"wb\") as file:\n",
    "            pickle.dump((test_preds, y_test), file)\n",
    "\n",
    "if save_model:\n",
    "    model.save(f\"../models/{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"../models/{name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_40\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_55 (InputLayer)       [(None, 220)]             0         \n",
      "                                                                 \n",
      " tf.expand_dims_13 (TFOpLamb  (None, 220, 1)           0         \n",
      " da)                                                             \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 220, 4)            96        \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 220, 8)            416       \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 220, 4)            208       \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 880)               0         \n",
      "                                                                 \n",
      " dense_154 (Dense)           (None, 32)                28192     \n",
      "                                                                 \n",
      " dropout_125 (Dropout)       (None, 32)                0         \n",
      "                                                                 \n",
      " dense_155 (Dense)           (None, 8)                 264       \n",
      "                                                                 \n",
      " dropout_126 (Dropout)       (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_156 (Dense)           (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,185\n",
      "Trainable params: 29,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "213/213 [==============================] - 213s 971ms/step - loss: 0.4810 - Acc: 0.7940 - AUC: 0.8527 - val_loss: 0.3939 - val_Acc: 0.8429 - val_AUC: 0.8996\n",
      "Epoch 2/50\n",
      "213/213 [==============================] - 146s 683ms/step - loss: 0.4194 - Acc: 0.8324 - AUC: 0.8879 - val_loss: 0.3809 - val_Acc: 0.8508 - val_AUC: 0.9071\n",
      "Epoch 3/50\n",
      "213/213 [==============================] - 82s 386ms/step - loss: 0.4111 - Acc: 0.8410 - AUC: 0.8912 - val_loss: 0.3821 - val_Acc: 0.8475 - val_AUC: 0.9069\n",
      "Epoch 4/50\n",
      "213/213 [==============================] - 91s 427ms/step - loss: 0.4067 - Acc: 0.8423 - AUC: 0.8955 - val_loss: 0.3791 - val_Acc: 0.8467 - val_AUC: 0.9068\n",
      "Epoch 5/50\n",
      "213/213 [==============================] - 101s 475ms/step - loss: 0.3986 - Acc: 0.8464 - AUC: 0.8975 - val_loss: 0.3812 - val_Acc: 0.8533 - val_AUC: 0.9076\n",
      "Epoch 6/50\n",
      "213/213 [==============================] - 106s 497ms/step - loss: 0.3929 - Acc: 0.8474 - AUC: 0.9001 - val_loss: 0.4270 - val_Acc: 0.8217 - val_AUC: 0.8829\n",
      "Epoch 7/50\n",
      "213/213 [==============================] - 96s 452ms/step - loss: 0.3978 - Acc: 0.8466 - AUC: 0.8969 - val_loss: 0.3701 - val_Acc: 0.8567 - val_AUC: 0.9101\n",
      "Epoch 8/50\n",
      "213/213 [==============================] - 104s 487ms/step - loss: 0.3884 - Acc: 0.8491 - AUC: 0.9023 - val_loss: 0.3674 - val_Acc: 0.8567 - val_AUC: 0.9139\n",
      "Epoch 9/50\n",
      "213/213 [==============================] - 110s 519ms/step - loss: 0.3906 - Acc: 0.8487 - AUC: 0.9017 - val_loss: 0.3698 - val_Acc: 0.8558 - val_AUC: 0.9137\n",
      "Epoch 10/50\n",
      "213/213 [==============================] - 127s 598ms/step - loss: 0.3860 - Acc: 0.8521 - AUC: 0.9032 - val_loss: 0.3685 - val_Acc: 0.8571 - val_AUC: 0.9107\n",
      "Epoch 11/50\n",
      "213/213 [==============================] - 123s 576ms/step - loss: 0.3866 - Acc: 0.8526 - AUC: 0.9035 - val_loss: 0.3903 - val_Acc: 0.8537 - val_AUC: 0.9113\n",
      "Epoch 12/50\n",
      "213/213 [==============================] - 106s 496ms/step - loss: 0.3892 - Acc: 0.8526 - AUC: 0.9025 - val_loss: 0.3629 - val_Acc: 0.8575 - val_AUC: 0.9145\n",
      "Epoch 13/50\n",
      "213/213 [==============================] - 108s 509ms/step - loss: 0.3820 - Acc: 0.8532 - AUC: 0.9058 - val_loss: 0.3591 - val_Acc: 0.8604 - val_AUC: 0.9171\n",
      "Epoch 14/50\n",
      "213/213 [==============================] - 116s 546ms/step - loss: 0.3827 - Acc: 0.8541 - AUC: 0.9060 - val_loss: 0.3831 - val_Acc: 0.8512 - val_AUC: 0.9043\n",
      "Epoch 15/50\n",
      "213/213 [==============================] - 113s 529ms/step - loss: 0.3761 - Acc: 0.8568 - AUC: 0.9093 - val_loss: 0.3645 - val_Acc: 0.8558 - val_AUC: 0.9145\n",
      "Epoch 16/50\n",
      "213/213 [==============================] - 105s 494ms/step - loss: 0.3764 - Acc: 0.8573 - AUC: 0.9079 - val_loss: 0.3619 - val_Acc: 0.8604 - val_AUC: 0.9132\n",
      "Epoch 17/50\n",
      "213/213 [==============================] - 204s 960ms/step - loss: 0.3755 - Acc: 0.8538 - AUC: 0.9101 - val_loss: 0.3667 - val_Acc: 0.8587 - val_AUC: 0.9116\n",
      "Epoch 18/50\n",
      "213/213 [==============================] - 254s 1s/step - loss: 0.3690 - Acc: 0.8577 - AUC: 0.9137 - val_loss: 0.3899 - val_Acc: 0.8537 - val_AUC: 0.9049\n",
      "Epoch 19/50\n",
      "213/213 [==============================] - 199s 937ms/step - loss: 0.3660 - Acc: 0.8583 - AUC: 0.9144 - val_loss: 0.3615 - val_Acc: 0.8612 - val_AUC: 0.9163\n",
      "Epoch 20/50\n",
      "213/213 [==============================] - 109s 512ms/step - loss: 0.3656 - Acc: 0.8598 - AUC: 0.9151 - val_loss: 0.3694 - val_Acc: 0.8617 - val_AUC: 0.9104\n",
      "Epoch 21/50\n",
      "213/213 [==============================] - 175s 811ms/step - loss: 0.3656 - Acc: 0.8602 - AUC: 0.9152 - val_loss: 0.3683 - val_Acc: 0.8550 - val_AUC: 0.9112\n",
      "Epoch 22/50\n",
      "213/213 [==============================] - 90s 421ms/step - loss: 0.3603 - Acc: 0.8598 - AUC: 0.9181 - val_loss: 0.3854 - val_Acc: 0.8446 - val_AUC: 0.9063\n",
      "Epoch 23/50\n",
      "213/213 [==============================] - 111s 522ms/step - loss: 0.3597 - Acc: 0.8612 - AUC: 0.9183 - val_loss: 0.3747 - val_Acc: 0.8596 - val_AUC: 0.9134\n",
      "125/125 [==============================] - 33s 245ms/step - loss: 0.3669 - Acc: 0.8585 - AUC: 0.9121\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "name           = \"LSTM\"\n",
    "dropout        = 0.15\n",
    "hidden_states  = 32 \n",
    "units          = (4, 8, 4)\n",
    "learning_rate  = 1e-2\n",
    "test_lr        = False\n",
    "test_interval  = (1e-9, 1)\n",
    "fit_model      = True\n",
    "test_model     = True\n",
    "save_preds     = True\n",
    "\n",
    "input = Input(shape = (timesteps))\n",
    "\n",
    "x = input\n",
    "x = tf.expand_dims(input, -1)\n",
    "\n",
    "for u in units:\n",
    "    x     = LSTM(u, dropout = dropout, return_sequences = True)(x)\n",
    "\n",
    "x     = Flatten()(x)\n",
    "\n",
    "x     = Dense(hidden_states, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "x     = Dense(hidden_states // 4, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "out   = Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs = input, outputs = out)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss    = BinaryCrossentropy(from_logits = False),\n",
    "    metrics   = [BinaryAccuracy(name = \"Acc\"), AUC(name = \"AUC\", from_logits = True)],\n",
    "    optimizer = Adam(learning_rate = learning_rate)\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "if test_lr:\n",
    "    finder = LRFinder(model)\n",
    "    finder.find(X_train_scaled, y_train, test_interval[0], test_interval[1], batch_size = batch_size)\n",
    "    finder.plot_loss()\n",
    "\n",
    "if fit_model:\n",
    "    model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs = epochs,\n",
    "        batch_size = batch_size,\n",
    "        validation_split = val_split,\n",
    "        callbacks = EarlyStopping(\"val_loss\", patience = patience, restore_best_weights = True)\n",
    "        )\n",
    "\n",
    "if test_model:\n",
    "    model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "    test_preds = model(X_test_scaled)\n",
    "\n",
    "    if save_preds:\n",
    "        with open(f\"../predictions/{name}.dat\", \"wb\") as file:\n",
    "            pickle.dump((test_preds, y_test), file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tcn import TCN, tcn_full_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_53\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_68 (InputLayer)       [(None, 220)]             0         \n",
      "                                                                 \n",
      " tf.expand_dims_26 (TFOpLamb  (None, 220, 1)           0         \n",
      " da)                                                             \n",
      "                                                                 \n",
      " tcn_16 (TCN)                (None, 220, 8)            2248      \n",
      "                                                                 \n",
      " flatten_18 (Flatten)        (None, 1760)              0         \n",
      "                                                                 \n",
      " dense_193 (Dense)           (None, 32)                56352     \n",
      "                                                                 \n",
      " dropout_151 (Dropout)       (None, 32)                0         \n",
      "                                                                 \n",
      " dense_194 (Dense)           (None, 8)                 264       \n",
      "                                                                 \n",
      " dropout_152 (Dropout)       (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_195 (Dense)           (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 58,873\n",
      "Trainable params: 58,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "213/213 [==============================] - 85s 318ms/step - loss: 1.1507 - Acc: 0.6089 - AUC: 0.6445 - val_loss: 0.6335 - val_Acc: 0.7150 - val_AUC: 0.7485\n",
      "Epoch 2/50\n",
      "213/213 [==============================] - 53s 249ms/step - loss: 0.6502 - Acc: 0.6817 - AUC: 0.7220 - val_loss: 0.6145 - val_Acc: 0.7208 - val_AUC: 0.8048\n",
      "Epoch 3/50\n",
      "213/213 [==============================] - 51s 238ms/step - loss: 0.6129 - Acc: 0.7140 - AUC: 0.7430 - val_loss: 0.5896 - val_Acc: 0.7433 - val_AUC: 0.8167\n",
      "Epoch 4/50\n",
      "213/213 [==============================] - 55s 256ms/step - loss: 0.5938 - Acc: 0.7273 - AUC: 0.7570 - val_loss: 0.5918 - val_Acc: 0.7208 - val_AUC: 0.8199\n",
      "Epoch 5/50\n",
      "213/213 [==============================] - 58s 272ms/step - loss: 0.5734 - Acc: 0.7424 - AUC: 0.7797 - val_loss: 0.5743 - val_Acc: 0.7267 - val_AUC: 0.8236\n",
      "Epoch 6/50\n",
      "213/213 [==============================] - 50s 235ms/step - loss: 0.5626 - Acc: 0.7523 - AUC: 0.7908 - val_loss: 0.5471 - val_Acc: 0.7571 - val_AUC: 0.8331\n",
      "Epoch 7/50\n",
      "213/213 [==============================] - 53s 250ms/step - loss: 0.5626 - Acc: 0.7530 - AUC: 0.7980 - val_loss: 0.5521 - val_Acc: 0.7429 - val_AUC: 0.8386\n",
      "Epoch 8/50\n",
      "213/213 [==============================] - 56s 263ms/step - loss: 0.5467 - Acc: 0.7584 - AUC: 0.8064 - val_loss: 0.5617 - val_Acc: 0.7237 - val_AUC: 0.8405\n",
      "Epoch 9/50\n",
      "213/213 [==============================] - 51s 240ms/step - loss: 0.5386 - Acc: 0.7626 - AUC: 0.8201 - val_loss: 0.5280 - val_Acc: 0.7592 - val_AUC: 0.8550\n",
      "Epoch 10/50\n",
      "213/213 [==============================] - 48s 227ms/step - loss: 0.5376 - Acc: 0.7671 - AUC: 0.8311 - val_loss: 0.5350 - val_Acc: 0.7371 - val_AUC: 0.8625\n",
      "Epoch 11/50\n",
      "213/213 [==============================] - 54s 256ms/step - loss: 0.5236 - Acc: 0.7713 - AUC: 0.8437 - val_loss: 0.5089 - val_Acc: 0.7558 - val_AUC: 0.8732\n",
      "Epoch 12/50\n",
      "213/213 [==============================] - 52s 243ms/step - loss: 0.5077 - Acc: 0.7796 - AUC: 0.8508 - val_loss: 0.5093 - val_Acc: 0.7571 - val_AUC: 0.8742\n",
      "Epoch 13/50\n",
      "213/213 [==============================] - 46s 217ms/step - loss: 0.5045 - Acc: 0.7787 - AUC: 0.8519 - val_loss: 0.5094 - val_Acc: 0.7613 - val_AUC: 0.8721\n",
      "Epoch 14/50\n",
      "213/213 [==============================] - 47s 220ms/step - loss: 0.4953 - Acc: 0.7865 - AUC: 0.8592 - val_loss: 0.4841 - val_Acc: 0.7917 - val_AUC: 0.8746\n",
      "Epoch 15/50\n",
      "213/213 [==============================] - 49s 231ms/step - loss: 0.4895 - Acc: 0.7922 - AUC: 0.8603 - val_loss: 0.4882 - val_Acc: 0.7842 - val_AUC: 0.8817\n",
      "Epoch 16/50\n",
      "213/213 [==============================] - 42s 195ms/step - loss: 0.4864 - Acc: 0.7997 - AUC: 0.8612 - val_loss: 0.4782 - val_Acc: 0.7954 - val_AUC: 0.8817\n",
      "Epoch 17/50\n",
      "213/213 [==============================] - 42s 196ms/step - loss: 0.4837 - Acc: 0.8007 - AUC: 0.8652 - val_loss: 0.4639 - val_Acc: 0.8171 - val_AUC: 0.8815\n",
      "Epoch 18/50\n",
      "213/213 [==============================] - 33s 156ms/step - loss: 0.4818 - Acc: 0.8032 - AUC: 0.8657 - val_loss: 0.4606 - val_Acc: 0.8179 - val_AUC: 0.8836\n",
      "Epoch 19/50\n",
      "213/213 [==============================] - 12s 55ms/step - loss: 0.4775 - Acc: 0.8021 - AUC: 0.8662 - val_loss: 0.4690 - val_Acc: 0.8087 - val_AUC: 0.8843\n",
      "Epoch 20/50\n",
      "213/213 [==============================] - 14s 64ms/step - loss: 0.4742 - Acc: 0.8052 - AUC: 0.8669 - val_loss: 0.4610 - val_Acc: 0.8121 - val_AUC: 0.8851\n",
      "Epoch 21/50\n",
      "213/213 [==============================] - 14s 67ms/step - loss: 0.4704 - Acc: 0.8107 - AUC: 0.8682 - val_loss: 0.4649 - val_Acc: 0.8100 - val_AUC: 0.8853\n",
      "125/125 [==============================] - 3s 15ms/step - loss: 0.4553 - Acc: 0.8077 - AUC: 0.8893\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "name           = \"TCN\"\n",
    "dropout        = 0.25\n",
    "hidden_states  = 32\n",
    "nb_filters     = 8\n",
    "kernel_size    = 3\n",
    "dilations      = (1, 2, 4, 8, 16, 32) \n",
    "learning_rate  = 1e-3\n",
    "test_lr        = False\n",
    "test_interval  = (1e-9, 1)\n",
    "fit_model      = True\n",
    "test_model     = True\n",
    "save_preds     = True\n",
    "\n",
    "input = Input(shape = (timesteps))\n",
    "\n",
    "x     = input\n",
    "x     = tf.expand_dims(input, -1)\n",
    "\n",
    "x     = TCN(\n",
    "    nb_filters  = nb_filters,\n",
    "    kernel_size = kernel_size,\n",
    "    dilations   = dilations,\n",
    "    dropout_rate = dropout,\n",
    "    return_sequences = True)(x)\n",
    "\n",
    "x     = Flatten()(x)\n",
    "\n",
    "x     = Dense(hidden_states, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "x     = Dense(hidden_states // 4, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "out   = Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs = input, outputs = out)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss    = BinaryCrossentropy(from_logits = False),\n",
    "    metrics   = [BinaryAccuracy(name = \"Acc\"), AUC(name = \"AUC\", from_logits = True)],\n",
    "    optimizer = Adam(learning_rate = learning_rate)\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "if test_lr:\n",
    "    finder = LRFinder(model)\n",
    "    finder.find(X_train_scaled, y_train, test_interval[0], test_interval[1], batch_size = batch_size)\n",
    "    finder.plot_loss()\n",
    "\n",
    "if fit_model:\n",
    "    model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs = epochs,\n",
    "        batch_size = batch_size,\n",
    "        validation_split = val_split,\n",
    "        callbacks = EarlyStopping(\"val_loss\", patience = patience, restore_best_weights = True)\n",
    "        )\n",
    "\n",
    "if test_model:\n",
    "    model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "    test_preds = model(X_test_scaled)\n",
    "\n",
    "    if save_preds:\n",
    "        with open(f\"../predictions/{name}.dat\", \"wb\") as file:\n",
    "            pickle.dump((test_preds, y_test), file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "609b8e94a29404947100e7ebc1571deda4e76c5e365f7a0289290abec32388f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
