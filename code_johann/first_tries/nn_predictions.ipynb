{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of Neural Networks on the Elzermann readout data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 09:25:55.987809: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-17 09:25:55.987857: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os, pickle\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Colors using scheme\n",
    "c = [\"#\" + i for i  in \"264653-2a9d8f-e9c46a-f4a261-e76f51\".split(\"-\")]\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "\n",
    "def fft_abs_phase(input):\n",
    "    transformed = np.fft.rfft(input)\n",
    "    output      = np.concatenate([np.abs(transformed), np.cos(np.angle(transformed)), np.sin(np.angle(transformed))],\n",
    "                                  axis = 1)\n",
    "    return output\n",
    "X_train, X_test, y_train, y_test = pd.read_pickle(\"/mnt/c/Users/johan/Desktop/CMT_project/data/train_test_data.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.metrics import AUC, BinaryAccuracy\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras_lr_finder import LRFinder\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs     = 50\n",
    "batch_size = 64\n",
    "timesteps  = 220\n",
    "val_split  = 0.10\n",
    "patience   = 10\n",
    "\n",
    "scaler              = RobustScaler()\n",
    "X_train_scaled      = scaler.fit_transform(X_train)\n",
    "X_test_scaled       = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_20 (InputLayer)       [(None, 220)]             0         \n",
      "                                                                 \n",
      " dense_90 (Dense)            (None, 64)                14144     \n",
      "                                                                 \n",
      " dropout_72 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_91 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_73 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_92 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_74 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_93 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_75 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_94 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,689\n",
      "Trainable params: 26,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "213/213 [==============================] - 3s 7ms/step - loss: 0.5463 - Acc: 0.7472 - AUC: 0.8131 - val_loss: 0.4990 - val_Acc: 0.7921 - val_AUC: 0.8557\n",
      "Epoch 2/50\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.4877 - Acc: 0.7832 - AUC: 0.8534 - val_loss: 0.4931 - val_Acc: 0.7942 - val_AUC: 0.8572\n",
      "Epoch 3/50\n",
      "213/213 [==============================] - 1s 7ms/step - loss: 0.4722 - Acc: 0.7932 - AUC: 0.8635 - val_loss: 0.4803 - val_Acc: 0.8050 - val_AUC: 0.8636\n",
      "Epoch 4/50\n",
      "213/213 [==============================] - 1s 7ms/step - loss: 0.4650 - Acc: 0.7925 - AUC: 0.8677 - val_loss: 0.4871 - val_Acc: 0.8000 - val_AUC: 0.8638\n",
      "Epoch 5/50\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.4588 - Acc: 0.7946 - AUC: 0.8708 - val_loss: 0.4826 - val_Acc: 0.7925 - val_AUC: 0.8641\n",
      "Epoch 6/50\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.4569 - Acc: 0.7991 - AUC: 0.8707 - val_loss: 0.4888 - val_Acc: 0.8029 - val_AUC: 0.8645\n",
      "Epoch 7/50\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.4469 - Acc: 0.8051 - AUC: 0.8775 - val_loss: 0.4725 - val_Acc: 0.7954 - val_AUC: 0.8673\n",
      "Epoch 8/50\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.4501 - Acc: 0.8063 - AUC: 0.8743 - val_loss: 0.4762 - val_Acc: 0.7983 - val_AUC: 0.8615\n",
      "Epoch 9/50\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.4425 - Acc: 0.8107 - AUC: 0.8779 - val_loss: 0.4821 - val_Acc: 0.7871 - val_AUC: 0.8590\n",
      "Epoch 10/50\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.4394 - Acc: 0.8055 - AUC: 0.8805 - val_loss: 0.4775 - val_Acc: 0.7971 - val_AUC: 0.8650\n",
      "Epoch 11/50\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.4341 - Acc: 0.8121 - AUC: 0.8838 - val_loss: 0.4784 - val_Acc: 0.7987 - val_AUC: 0.8645\n",
      "Epoch 12/50\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.4314 - Acc: 0.8153 - AUC: 0.8859 - val_loss: 0.4782 - val_Acc: 0.7962 - val_AUC: 0.8629\n",
      "125/125 [==============================] - 1s 3ms/step - loss: 0.4726 - Acc: 0.7972 - AUC: 0.8706\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "name           = \"feed_forward_network\"\n",
    "dropout        = 0.25\n",
    "hidden_states  = 64 \n",
    "learning_rate  = 5e-3\n",
    "test_lr        = False\n",
    "test_interval  = (1e-9, 1)\n",
    "fit_model      = True\n",
    "test_model     = True\n",
    "save_preds     = True\n",
    "\n",
    "input = Input(shape = (timesteps))\n",
    "\n",
    "x     = Dense(hidden_states, activation = \"relu\")(input)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "x     = Dense(hidden_states, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "x     = Dense(hidden_states, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "x     = Dense(hidden_states, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "out   = Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs = input, outputs = out)\n",
    "\n",
    "model.compile(\n",
    "    loss    = BinaryCrossentropy(from_logits = False),\n",
    "    metrics   = [BinaryAccuracy(name = \"Acc\"), AUC(name = \"AUC\", from_logits = True)],\n",
    "    optimizer = Adam(learning_rate = learning_rate)\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "if test_lr:\n",
    "    finder = LRFinder(model)\n",
    "    finder.find(X_train_scaled, y_train, test_interval[0], test_interval[1], batch_size = batch_size)\n",
    "    finder.plot_loss()\n",
    "\n",
    "if fit_model:\n",
    "    model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs = epochs,\n",
    "        batch_size = batch_size,\n",
    "        validation_split = val_split,\n",
    "        callbacks = EarlyStopping(\"val_loss\", patience = patience, restore_best_weights = True)\n",
    "        )\n",
    "\n",
    "if test_model:\n",
    "    model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "    test_preds = model(X_test_scaled)\n",
    "\n",
    "    if save_preds:\n",
    "        with open(f\"../predictions/{name}.dat\", \"wb\") as file:\n",
    "            pickle.dump((test_preds, y_test), file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D-conv:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, Flatten, Normalization\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "lr_schedule_func = lambda epoch, lr: lr if epoch < 5 else 0.90 * lr\n",
    "\n",
    "# qnt_scaler = QuantileTransformer()\n",
    "# qnt_scaler.fit(X_train)\n",
    "\n",
    "# norm_layer = Normalization(mean = tf.convert_to_tensor(scaler.center_, dtype = tf.float32), variance = tf.convert_to_tensor(scaler.scale_ ** 2, dtype = tf.float32))\n",
    "norm_layer = Normalization()\n",
    "norm_layer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_28 (InputLayer)       [(None, 220)]             0         \n",
      "                                                                 \n",
      " normalization_16 (Normaliza  (None, 220)              441       \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " tf.expand_dims_27 (TFOpLamb  (None, 220, 1)           0         \n",
      " da)                                                             \n",
      "                                                                 \n",
      " conv1d_102 (Conv1D)         (None, 120, 32)           3264      \n",
      "                                                                 \n",
      " conv1d_103 (Conv1D)         (None, 70, 16)            26128     \n",
      "                                                                 \n",
      " conv1d_104 (Conv1D)         (None, 46, 16)            6416      \n",
      "                                                                 \n",
      " conv1d_105 (Conv1D)         (None, 37, 8)             1288      \n",
      "                                                                 \n",
      " flatten_25 (Flatten)        (None, 296)               0         \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 32)                9504      \n",
      "                                                                 \n",
      " dropout_50 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_76 (Dense)            (None, 8)                 264       \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 47,314\n",
      "Trainable params: 46,873\n",
      "Non-trainable params: 441\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "225/225 [==============================] - 5s 21ms/step - loss: 0.5774 - Acc: 0.6893 - AUC: 0.7844 - val_loss: 0.4525 - val_Acc: 0.8194 - val_AUC: 0.8841 - lr: 5.0000e-04\n",
      "Epoch 2/50\n",
      "225/225 [==============================] - 4s 18ms/step - loss: 0.5142 - Acc: 0.7822 - AUC: 0.8457 - val_loss: 0.4448 - val_Acc: 0.8250 - val_AUC: 0.8899 - lr: 5.0000e-04\n",
      "Epoch 3/50\n",
      "225/225 [==============================] - 4s 20ms/step - loss: 0.4870 - Acc: 0.7962 - AUC: 0.8605 - val_loss: 0.4211 - val_Acc: 0.8281 - val_AUC: 0.8907 - lr: 5.0000e-04\n",
      "Epoch 4/50\n",
      "225/225 [==============================] - 5s 21ms/step - loss: 0.4757 - Acc: 0.8052 - AUC: 0.8682 - val_loss: 0.4179 - val_Acc: 0.8394 - val_AUC: 0.9019 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.4684 - Acc: 0.8109 - AUC: 0.8740 - val_loss: 0.4044 - val_Acc: 0.8363 - val_AUC: 0.9020 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.4628 - Acc: 0.8203 - AUC: 0.8775 - val_loss: 0.4235 - val_Acc: 0.8475 - val_AUC: 0.9041 - lr: 4.5000e-04\n",
      "Epoch 7/50\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.4515 - Acc: 0.8263 - AUC: 0.8829 - val_loss: 0.3952 - val_Acc: 0.8569 - val_AUC: 0.9085 - lr: 4.0500e-04\n",
      "Epoch 8/50\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.4500 - Acc: 0.8289 - AUC: 0.8831 - val_loss: 0.3814 - val_Acc: 0.8550 - val_AUC: 0.9079 - lr: 3.6450e-04\n",
      "Epoch 9/50\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.4418 - Acc: 0.8306 - AUC: 0.8874 - val_loss: 0.3837 - val_Acc: 0.8550 - val_AUC: 0.9072 - lr: 3.2805e-04\n",
      "Epoch 10/50\n",
      "225/225 [==============================] - 5s 21ms/step - loss: 0.4359 - Acc: 0.8317 - AUC: 0.8881 - val_loss: 0.3744 - val_Acc: 0.8562 - val_AUC: 0.9126 - lr: 2.9525e-04\n",
      "Epoch 11/50\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.4358 - Acc: 0.8363 - AUC: 0.8901 - val_loss: 0.3734 - val_Acc: 0.8581 - val_AUC: 0.9126 - lr: 2.6572e-04\n",
      "Epoch 12/50\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.4307 - Acc: 0.8406 - AUC: 0.8937 - val_loss: 0.3721 - val_Acc: 0.8531 - val_AUC: 0.9100 - lr: 2.3915e-04\n",
      "Epoch 13/50\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.4241 - Acc: 0.8412 - AUC: 0.8955 - val_loss: 0.3794 - val_Acc: 0.8556 - val_AUC: 0.9088 - lr: 2.1523e-04\n",
      "Epoch 14/50\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.4232 - Acc: 0.8440 - AUC: 0.8961 - val_loss: 0.3710 - val_Acc: 0.8587 - val_AUC: 0.9112 - lr: 1.9371e-04\n",
      "Epoch 15/50\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.4188 - Acc: 0.8450 - AUC: 0.8991 - val_loss: 0.3706 - val_Acc: 0.8537 - val_AUC: 0.9118 - lr: 1.7434e-04\n",
      "Epoch 16/50\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.4123 - Acc: 0.8480 - AUC: 0.9016 - val_loss: 0.3805 - val_Acc: 0.8512 - val_AUC: 0.9094 - lr: 1.5691e-04\n",
      "Epoch 17/50\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.4120 - Acc: 0.8482 - AUC: 0.9013 - val_loss: 0.3709 - val_Acc: 0.8600 - val_AUC: 0.9115 - lr: 1.4121e-04\n",
      "Epoch 18/50\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.4040 - Acc: 0.8537 - AUC: 0.9042 - val_loss: 0.3774 - val_Acc: 0.8562 - val_AUC: 0.9101 - lr: 1.2709e-04\n",
      "Epoch 19/50\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.4027 - Acc: 0.8539 - AUC: 0.9046 - val_loss: 0.3752 - val_Acc: 0.8569 - val_AUC: 0.9096 - lr: 1.1438e-04\n",
      "Epoch 20/50\n",
      "225/225 [==============================] - 5s 23ms/step - loss: 0.4043 - Acc: 0.8566 - AUC: 0.9047 - val_loss: 0.3800 - val_Acc: 0.8587 - val_AUC: 0.9080 - lr: 1.0295e-04\n",
      "Epoch 21/50\n",
      "225/225 [==============================] - 5s 24ms/step - loss: 0.3993 - Acc: 0.8587 - AUC: 0.9076 - val_loss: 0.3730 - val_Acc: 0.8575 - val_AUC: 0.9100 - lr: 9.2651e-05\n",
      "Epoch 22/50\n",
      "225/225 [==============================] - 5s 23ms/step - loss: 0.3956 - Acc: 0.8615 - AUC: 0.9084 - val_loss: 0.3811 - val_Acc: 0.8519 - val_AUC: 0.9082 - lr: 8.3386e-05\n",
      "Epoch 23/50\n",
      "225/225 [==============================] - 5s 23ms/step - loss: 0.3909 - Acc: 0.8606 - AUC: 0.9099 - val_loss: 0.3751 - val_Acc: 0.8569 - val_AUC: 0.9094 - lr: 7.5047e-05\n",
      "Epoch 24/50\n",
      "225/225 [==============================] - 5s 23ms/step - loss: 0.3941 - Acc: 0.8626 - AUC: 0.9099 - val_loss: 0.3827 - val_Acc: 0.8519 - val_AUC: 0.9077 - lr: 6.7543e-05\n",
      "Epoch 25/50\n",
      "225/225 [==============================] - 5s 23ms/step - loss: 0.3882 - Acc: 0.8655 - AUC: 0.9116 - val_loss: 0.3778 - val_Acc: 0.8531 - val_AUC: 0.9091 - lr: 6.0788e-05\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.3702 - Acc: 0.8525 - AUC: 0.9122\n",
      "INFO:tensorflow:Assets written to: ../models/1D_conv/assets\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "name           = \"1D_conv\"\n",
    "dropout        = 0.5\n",
    "filter_sizes   = (32,  16, 16, 8)\n",
    "kernal_sizes   = (101, 51, 25, 10)\n",
    "hidden_states  = 32\n",
    "learning_rate  = 5e-4\n",
    "test_lr        = False\n",
    "test_interval  = (1e-9, 10)\n",
    "fit_model      = True\n",
    "test_model     = True\n",
    "save_preds     = True\n",
    "save_model     = True\n",
    "\n",
    "\n",
    "input = Input(shape = (timesteps))\n",
    "\n",
    "x = norm_layer(input)\n",
    "\n",
    "x = tf.expand_dims(x, -1)\n",
    "\n",
    "for fs, ks in zip(filter_sizes, kernal_sizes):\n",
    "    x     = Conv1D(fs, kernel_size = ks, activation = \"relu\")(x)\n",
    "\n",
    "x     = Flatten()(x)\n",
    "\n",
    "# x     = tf.squeeze(x, -1)\n",
    "\n",
    "x     = Dense(hidden_states, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "x     = Dense(hidden_states // 4, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "out   = Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs = input, outputs = out)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss    = BinaryCrossentropy(from_logits = False),\n",
    "    metrics   = [BinaryAccuracy(name = \"Acc\"), AUC(name = \"AUC\", from_logits = True)],\n",
    "    optimizer = Adam(learning_rate = learning_rate)\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "if test_lr:\n",
    "    finder = LRFinder(model)\n",
    "    finder.find(X_train_scaled, y_train, test_interval[0], test_interval[1], batch_size = batch_size)\n",
    "    finder.plot_loss()\n",
    "\n",
    "if fit_model:\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs = epochs,\n",
    "        batch_size = batch_size,\n",
    "        validation_split = val_split,\n",
    "        callbacks = [LearningRateScheduler(lr_schedule_func, verbose = 0),\n",
    "                     EarlyStopping(\"val_loss\", patience = patience, restore_best_weights = True)]\n",
    "        )\n",
    "\n",
    "if test_model:\n",
    "    model.evaluate(X_test, y_test)\n",
    "\n",
    "    test_preds = model(X_test)\n",
    "\n",
    "    if save_preds:\n",
    "        with open(f\"../predictions/{name}.dat\", \"wb\") as file:\n",
    "            pickle.dump((test_preds, y_test), file)\n",
    "\n",
    "if save_model:\n",
    "    model.save(f\"../models/{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"../models/{name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_40\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_55 (InputLayer)       [(None, 220)]             0         \n",
      "                                                                 \n",
      " tf.expand_dims_13 (TFOpLamb  (None, 220, 1)           0         \n",
      " da)                                                             \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 220, 4)            96        \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 220, 8)            416       \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 220, 4)            208       \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 880)               0         \n",
      "                                                                 \n",
      " dense_154 (Dense)           (None, 32)                28192     \n",
      "                                                                 \n",
      " dropout_125 (Dropout)       (None, 32)                0         \n",
      "                                                                 \n",
      " dense_155 (Dense)           (None, 8)                 264       \n",
      "                                                                 \n",
      " dropout_126 (Dropout)       (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_156 (Dense)           (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,185\n",
      "Trainable params: 29,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "213/213 [==============================] - 213s 971ms/step - loss: 0.4810 - Acc: 0.7940 - AUC: 0.8527 - val_loss: 0.3939 - val_Acc: 0.8429 - val_AUC: 0.8996\n",
      "Epoch 2/50\n",
      "213/213 [==============================] - 146s 683ms/step - loss: 0.4194 - Acc: 0.8324 - AUC: 0.8879 - val_loss: 0.3809 - val_Acc: 0.8508 - val_AUC: 0.9071\n",
      "Epoch 3/50\n",
      "213/213 [==============================] - 82s 386ms/step - loss: 0.4111 - Acc: 0.8410 - AUC: 0.8912 - val_loss: 0.3821 - val_Acc: 0.8475 - val_AUC: 0.9069\n",
      "Epoch 4/50\n",
      "213/213 [==============================] - 91s 427ms/step - loss: 0.4067 - Acc: 0.8423 - AUC: 0.8955 - val_loss: 0.3791 - val_Acc: 0.8467 - val_AUC: 0.9068\n",
      "Epoch 5/50\n",
      "213/213 [==============================] - 101s 475ms/step - loss: 0.3986 - Acc: 0.8464 - AUC: 0.8975 - val_loss: 0.3812 - val_Acc: 0.8533 - val_AUC: 0.9076\n",
      "Epoch 6/50\n",
      "213/213 [==============================] - 106s 497ms/step - loss: 0.3929 - Acc: 0.8474 - AUC: 0.9001 - val_loss: 0.4270 - val_Acc: 0.8217 - val_AUC: 0.8829\n",
      "Epoch 7/50\n",
      "213/213 [==============================] - 96s 452ms/step - loss: 0.3978 - Acc: 0.8466 - AUC: 0.8969 - val_loss: 0.3701 - val_Acc: 0.8567 - val_AUC: 0.9101\n",
      "Epoch 8/50\n",
      "213/213 [==============================] - 104s 487ms/step - loss: 0.3884 - Acc: 0.8491 - AUC: 0.9023 - val_loss: 0.3674 - val_Acc: 0.8567 - val_AUC: 0.9139\n",
      "Epoch 9/50\n",
      "213/213 [==============================] - 110s 519ms/step - loss: 0.3906 - Acc: 0.8487 - AUC: 0.9017 - val_loss: 0.3698 - val_Acc: 0.8558 - val_AUC: 0.9137\n",
      "Epoch 10/50\n",
      "213/213 [==============================] - 127s 598ms/step - loss: 0.3860 - Acc: 0.8521 - AUC: 0.9032 - val_loss: 0.3685 - val_Acc: 0.8571 - val_AUC: 0.9107\n",
      "Epoch 11/50\n",
      "213/213 [==============================] - 123s 576ms/step - loss: 0.3866 - Acc: 0.8526 - AUC: 0.9035 - val_loss: 0.3903 - val_Acc: 0.8537 - val_AUC: 0.9113\n",
      "Epoch 12/50\n",
      "213/213 [==============================] - 106s 496ms/step - loss: 0.3892 - Acc: 0.8526 - AUC: 0.9025 - val_loss: 0.3629 - val_Acc: 0.8575 - val_AUC: 0.9145\n",
      "Epoch 13/50\n",
      "213/213 [==============================] - 108s 509ms/step - loss: 0.3820 - Acc: 0.8532 - AUC: 0.9058 - val_loss: 0.3591 - val_Acc: 0.8604 - val_AUC: 0.9171\n",
      "Epoch 14/50\n",
      "213/213 [==============================] - 116s 546ms/step - loss: 0.3827 - Acc: 0.8541 - AUC: 0.9060 - val_loss: 0.3831 - val_Acc: 0.8512 - val_AUC: 0.9043\n",
      "Epoch 15/50\n",
      "213/213 [==============================] - 113s 529ms/step - loss: 0.3761 - Acc: 0.8568 - AUC: 0.9093 - val_loss: 0.3645 - val_Acc: 0.8558 - val_AUC: 0.9145\n",
      "Epoch 16/50\n",
      "213/213 [==============================] - 105s 494ms/step - loss: 0.3764 - Acc: 0.8573 - AUC: 0.9079 - val_loss: 0.3619 - val_Acc: 0.8604 - val_AUC: 0.9132\n",
      "Epoch 17/50\n",
      "213/213 [==============================] - 204s 960ms/step - loss: 0.3755 - Acc: 0.8538 - AUC: 0.9101 - val_loss: 0.3667 - val_Acc: 0.8587 - val_AUC: 0.9116\n",
      "Epoch 18/50\n",
      "213/213 [==============================] - 254s 1s/step - loss: 0.3690 - Acc: 0.8577 - AUC: 0.9137 - val_loss: 0.3899 - val_Acc: 0.8537 - val_AUC: 0.9049\n",
      "Epoch 19/50\n",
      "213/213 [==============================] - 199s 937ms/step - loss: 0.3660 - Acc: 0.8583 - AUC: 0.9144 - val_loss: 0.3615 - val_Acc: 0.8612 - val_AUC: 0.9163\n",
      "Epoch 20/50\n",
      "213/213 [==============================] - 109s 512ms/step - loss: 0.3656 - Acc: 0.8598 - AUC: 0.9151 - val_loss: 0.3694 - val_Acc: 0.8617 - val_AUC: 0.9104\n",
      "Epoch 21/50\n",
      "213/213 [==============================] - 175s 811ms/step - loss: 0.3656 - Acc: 0.8602 - AUC: 0.9152 - val_loss: 0.3683 - val_Acc: 0.8550 - val_AUC: 0.9112\n",
      "Epoch 22/50\n",
      "213/213 [==============================] - 90s 421ms/step - loss: 0.3603 - Acc: 0.8598 - AUC: 0.9181 - val_loss: 0.3854 - val_Acc: 0.8446 - val_AUC: 0.9063\n",
      "Epoch 23/50\n",
      "213/213 [==============================] - 111s 522ms/step - loss: 0.3597 - Acc: 0.8612 - AUC: 0.9183 - val_loss: 0.3747 - val_Acc: 0.8596 - val_AUC: 0.9134\n",
      "125/125 [==============================] - 33s 245ms/step - loss: 0.3669 - Acc: 0.8585 - AUC: 0.9121\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "name           = \"LSTM\"\n",
    "dropout        = 0.15\n",
    "hidden_states  = 32 \n",
    "units          = (4, 8, 4)\n",
    "learning_rate  = 1e-2\n",
    "test_lr        = False\n",
    "test_interval  = (1e-9, 1)\n",
    "fit_model      = True\n",
    "test_model     = True\n",
    "save_preds     = True\n",
    "\n",
    "input = Input(shape = (timesteps))\n",
    "\n",
    "x = input\n",
    "x = tf.expand_dims(input, -1)\n",
    "\n",
    "for u in units:\n",
    "    x     = LSTM(u, dropout = dropout, return_sequences = True)(x)\n",
    "\n",
    "x     = Flatten()(x)\n",
    "\n",
    "x     = Dense(hidden_states, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "x     = Dense(hidden_states // 4, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "out   = Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs = input, outputs = out)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss    = BinaryCrossentropy(from_logits = False),\n",
    "    metrics   = [BinaryAccuracy(name = \"Acc\"), AUC(name = \"AUC\", from_logits = True)],\n",
    "    optimizer = Adam(learning_rate = learning_rate)\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "if test_lr:\n",
    "    finder = LRFinder(model)\n",
    "    finder.find(X_train_scaled, y_train, test_interval[0], test_interval[1], batch_size = batch_size)\n",
    "    finder.plot_loss()\n",
    "\n",
    "if fit_model:\n",
    "    model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs = epochs,\n",
    "        batch_size = batch_size,\n",
    "        validation_split = val_split,\n",
    "        callbacks = EarlyStopping(\"val_loss\", patience = patience, restore_best_weights = True)\n",
    "        )\n",
    "\n",
    "if test_model:\n",
    "    model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "    test_preds = model(X_test_scaled)\n",
    "\n",
    "    if save_preds:\n",
    "        with open(f\"../predictions/{name}.dat\", \"wb\") as file:\n",
    "            pickle.dump((test_preds, y_test), file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tcn import TCN, tcn_full_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_53\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_68 (InputLayer)       [(None, 220)]             0         \n",
      "                                                                 \n",
      " tf.expand_dims_26 (TFOpLamb  (None, 220, 1)           0         \n",
      " da)                                                             \n",
      "                                                                 \n",
      " tcn_16 (TCN)                (None, 220, 8)            2248      \n",
      "                                                                 \n",
      " flatten_18 (Flatten)        (None, 1760)              0         \n",
      "                                                                 \n",
      " dense_193 (Dense)           (None, 32)                56352     \n",
      "                                                                 \n",
      " dropout_151 (Dropout)       (None, 32)                0         \n",
      "                                                                 \n",
      " dense_194 (Dense)           (None, 8)                 264       \n",
      "                                                                 \n",
      " dropout_152 (Dropout)       (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_195 (Dense)           (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 58,873\n",
      "Trainable params: 58,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "213/213 [==============================] - 85s 318ms/step - loss: 1.1507 - Acc: 0.6089 - AUC: 0.6445 - val_loss: 0.6335 - val_Acc: 0.7150 - val_AUC: 0.7485\n",
      "Epoch 2/50\n",
      "213/213 [==============================] - 53s 249ms/step - loss: 0.6502 - Acc: 0.6817 - AUC: 0.7220 - val_loss: 0.6145 - val_Acc: 0.7208 - val_AUC: 0.8048\n",
      "Epoch 3/50\n",
      "213/213 [==============================] - 51s 238ms/step - loss: 0.6129 - Acc: 0.7140 - AUC: 0.7430 - val_loss: 0.5896 - val_Acc: 0.7433 - val_AUC: 0.8167\n",
      "Epoch 4/50\n",
      "213/213 [==============================] - 55s 256ms/step - loss: 0.5938 - Acc: 0.7273 - AUC: 0.7570 - val_loss: 0.5918 - val_Acc: 0.7208 - val_AUC: 0.8199\n",
      "Epoch 5/50\n",
      "213/213 [==============================] - 58s 272ms/step - loss: 0.5734 - Acc: 0.7424 - AUC: 0.7797 - val_loss: 0.5743 - val_Acc: 0.7267 - val_AUC: 0.8236\n",
      "Epoch 6/50\n",
      "213/213 [==============================] - 50s 235ms/step - loss: 0.5626 - Acc: 0.7523 - AUC: 0.7908 - val_loss: 0.5471 - val_Acc: 0.7571 - val_AUC: 0.8331\n",
      "Epoch 7/50\n",
      "213/213 [==============================] - 53s 250ms/step - loss: 0.5626 - Acc: 0.7530 - AUC: 0.7980 - val_loss: 0.5521 - val_Acc: 0.7429 - val_AUC: 0.8386\n",
      "Epoch 8/50\n",
      "213/213 [==============================] - 56s 263ms/step - loss: 0.5467 - Acc: 0.7584 - AUC: 0.8064 - val_loss: 0.5617 - val_Acc: 0.7237 - val_AUC: 0.8405\n",
      "Epoch 9/50\n",
      "213/213 [==============================] - 51s 240ms/step - loss: 0.5386 - Acc: 0.7626 - AUC: 0.8201 - val_loss: 0.5280 - val_Acc: 0.7592 - val_AUC: 0.8550\n",
      "Epoch 10/50\n",
      "213/213 [==============================] - 48s 227ms/step - loss: 0.5376 - Acc: 0.7671 - AUC: 0.8311 - val_loss: 0.5350 - val_Acc: 0.7371 - val_AUC: 0.8625\n",
      "Epoch 11/50\n",
      "213/213 [==============================] - 54s 256ms/step - loss: 0.5236 - Acc: 0.7713 - AUC: 0.8437 - val_loss: 0.5089 - val_Acc: 0.7558 - val_AUC: 0.8732\n",
      "Epoch 12/50\n",
      "213/213 [==============================] - 52s 243ms/step - loss: 0.5077 - Acc: 0.7796 - AUC: 0.8508 - val_loss: 0.5093 - val_Acc: 0.7571 - val_AUC: 0.8742\n",
      "Epoch 13/50\n",
      "213/213 [==============================] - 46s 217ms/step - loss: 0.5045 - Acc: 0.7787 - AUC: 0.8519 - val_loss: 0.5094 - val_Acc: 0.7613 - val_AUC: 0.8721\n",
      "Epoch 14/50\n",
      "213/213 [==============================] - 47s 220ms/step - loss: 0.4953 - Acc: 0.7865 - AUC: 0.8592 - val_loss: 0.4841 - val_Acc: 0.7917 - val_AUC: 0.8746\n",
      "Epoch 15/50\n",
      "213/213 [==============================] - 49s 231ms/step - loss: 0.4895 - Acc: 0.7922 - AUC: 0.8603 - val_loss: 0.4882 - val_Acc: 0.7842 - val_AUC: 0.8817\n",
      "Epoch 16/50\n",
      "213/213 [==============================] - 42s 195ms/step - loss: 0.4864 - Acc: 0.7997 - AUC: 0.8612 - val_loss: 0.4782 - val_Acc: 0.7954 - val_AUC: 0.8817\n",
      "Epoch 17/50\n",
      "213/213 [==============================] - 42s 196ms/step - loss: 0.4837 - Acc: 0.8007 - AUC: 0.8652 - val_loss: 0.4639 - val_Acc: 0.8171 - val_AUC: 0.8815\n",
      "Epoch 18/50\n",
      "213/213 [==============================] - 33s 156ms/step - loss: 0.4818 - Acc: 0.8032 - AUC: 0.8657 - val_loss: 0.4606 - val_Acc: 0.8179 - val_AUC: 0.8836\n",
      "Epoch 19/50\n",
      "213/213 [==============================] - 12s 55ms/step - loss: 0.4775 - Acc: 0.8021 - AUC: 0.8662 - val_loss: 0.4690 - val_Acc: 0.8087 - val_AUC: 0.8843\n",
      "Epoch 20/50\n",
      "213/213 [==============================] - 14s 64ms/step - loss: 0.4742 - Acc: 0.8052 - AUC: 0.8669 - val_loss: 0.4610 - val_Acc: 0.8121 - val_AUC: 0.8851\n",
      "Epoch 21/50\n",
      "213/213 [==============================] - 14s 67ms/step - loss: 0.4704 - Acc: 0.8107 - AUC: 0.8682 - val_loss: 0.4649 - val_Acc: 0.8100 - val_AUC: 0.8853\n",
      "125/125 [==============================] - 3s 15ms/step - loss: 0.4553 - Acc: 0.8077 - AUC: 0.8893\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "name           = \"TCN\"\n",
    "dropout        = 0.25\n",
    "hidden_states  = 32\n",
    "nb_filters     = 8\n",
    "kernel_size    = 3\n",
    "dilations      = (1, 2, 4, 8, 16, 32) \n",
    "learning_rate  = 1e-3\n",
    "test_lr        = False\n",
    "test_interval  = (1e-9, 1)\n",
    "fit_model      = True\n",
    "test_model     = True\n",
    "save_preds     = True\n",
    "\n",
    "input = Input(shape = (timesteps))\n",
    "\n",
    "x     = input\n",
    "x     = tf.expand_dims(input, -1)\n",
    "\n",
    "x     = TCN(\n",
    "    nb_filters  = nb_filters,\n",
    "    kernel_size = kernel_size,\n",
    "    dilations   = dilations,\n",
    "    dropout_rate = dropout,\n",
    "    return_sequences = True)(x)\n",
    "\n",
    "x     = Flatten()(x)\n",
    "\n",
    "x     = Dense(hidden_states, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "x     = Dense(hidden_states // 4, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "out   = Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs = input, outputs = out)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss    = BinaryCrossentropy(from_logits = False),\n",
    "    metrics   = [BinaryAccuracy(name = \"Acc\"), AUC(name = \"AUC\", from_logits = True)],\n",
    "    optimizer = Adam(learning_rate = learning_rate)\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "if test_lr:\n",
    "    finder = LRFinder(model)\n",
    "    finder.find(X_train_scaled, y_train, test_interval[0], test_interval[1], batch_size = batch_size)\n",
    "    finder.plot_loss()\n",
    "\n",
    "if fit_model:\n",
    "    model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs = epochs,\n",
    "        batch_size = batch_size,\n",
    "        validation_split = val_split,\n",
    "        callbacks = EarlyStopping(\"val_loss\", patience = patience, restore_best_weights = True)\n",
    "        )\n",
    "\n",
    "if test_model:\n",
    "    model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "    test_preds = model(X_test_scaled)\n",
    "\n",
    "    if save_preds:\n",
    "        with open(f\"../predictions/{name}.dat\", \"wb\") as file:\n",
    "            pickle.dump((test_preds, y_test), file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "609b8e94a29404947100e7ebc1571deda4e76c5e365f7a0289290abec32388f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
