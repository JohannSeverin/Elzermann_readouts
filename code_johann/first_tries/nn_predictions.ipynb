{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of Neural Networks on the Elzermann readout data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Colors using scheme\n",
    "c = [\"#\" + i for i  in \"264653-2a9d8f-e9c46a-f4a261-e76f51\".split(\"-\")]\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "\n",
    "def fft_abs_phase(input):\n",
    "    transformed = np.fft.rfft(input)\n",
    "    output      = np.concatenate([np.abs(transformed), np.cos(np.angle(transformed)), np.sin(np.angle(transformed))],\n",
    "                                  axis = 1)\n",
    "    return output\n",
    "X_train, X_test, y_train, y_test = pd.read_pickle(\"/mnt/c/Users/johan/Desktop/CMT_project/data/train_test_data.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.metrics import AUC, BinaryAccuracy\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras_lr_finder import LRFinder\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs     = 50\n",
    "batch_size = 64\n",
    "timesteps  = 220\n",
    "val_split  = 0.15\n",
    "patience   = 3\n",
    "\n",
    "scaler              = RobustScaler()\n",
    "X_train_scaled      = scaler.fit_transform(X_train)\n",
    "X_test_scaled       = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_20 (InputLayer)       [(None, 220)]             0         \n",
      "                                                                 \n",
      " dense_90 (Dense)            (None, 64)                14144     \n",
      "                                                                 \n",
      " dropout_72 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_91 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_73 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_92 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_74 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_93 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_75 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_94 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,689\n",
      "Trainable params: 26,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "213/213 [==============================] - 3s 7ms/step - loss: 0.5463 - Acc: 0.7472 - AUC: 0.8131 - val_loss: 0.4990 - val_Acc: 0.7921 - val_AUC: 0.8557\n",
      "Epoch 2/50\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.4877 - Acc: 0.7832 - AUC: 0.8534 - val_loss: 0.4931 - val_Acc: 0.7942 - val_AUC: 0.8572\n",
      "Epoch 3/50\n",
      "213/213 [==============================] - 1s 7ms/step - loss: 0.4722 - Acc: 0.7932 - AUC: 0.8635 - val_loss: 0.4803 - val_Acc: 0.8050 - val_AUC: 0.8636\n",
      "Epoch 4/50\n",
      "213/213 [==============================] - 1s 7ms/step - loss: 0.4650 - Acc: 0.7925 - AUC: 0.8677 - val_loss: 0.4871 - val_Acc: 0.8000 - val_AUC: 0.8638\n",
      "Epoch 5/50\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.4588 - Acc: 0.7946 - AUC: 0.8708 - val_loss: 0.4826 - val_Acc: 0.7925 - val_AUC: 0.8641\n",
      "Epoch 6/50\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.4569 - Acc: 0.7991 - AUC: 0.8707 - val_loss: 0.4888 - val_Acc: 0.8029 - val_AUC: 0.8645\n",
      "Epoch 7/50\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.4469 - Acc: 0.8051 - AUC: 0.8775 - val_loss: 0.4725 - val_Acc: 0.7954 - val_AUC: 0.8673\n",
      "Epoch 8/50\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.4501 - Acc: 0.8063 - AUC: 0.8743 - val_loss: 0.4762 - val_Acc: 0.7983 - val_AUC: 0.8615\n",
      "Epoch 9/50\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.4425 - Acc: 0.8107 - AUC: 0.8779 - val_loss: 0.4821 - val_Acc: 0.7871 - val_AUC: 0.8590\n",
      "Epoch 10/50\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.4394 - Acc: 0.8055 - AUC: 0.8805 - val_loss: 0.4775 - val_Acc: 0.7971 - val_AUC: 0.8650\n",
      "Epoch 11/50\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.4341 - Acc: 0.8121 - AUC: 0.8838 - val_loss: 0.4784 - val_Acc: 0.7987 - val_AUC: 0.8645\n",
      "Epoch 12/50\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.4314 - Acc: 0.8153 - AUC: 0.8859 - val_loss: 0.4782 - val_Acc: 0.7962 - val_AUC: 0.8629\n",
      "125/125 [==============================] - 1s 3ms/step - loss: 0.4726 - Acc: 0.7972 - AUC: 0.8706\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "name           = \"feed_forward_network\"\n",
    "dropout        = 0.25\n",
    "hidden_states  = 64 \n",
    "learning_rate  = 5e-3\n",
    "test_lr        = False\n",
    "test_interval  = (1e-9, 1)\n",
    "fit_model      = True\n",
    "test_model     = True\n",
    "save_preds     = True\n",
    "\n",
    "input = Input(shape = (timesteps))\n",
    "\n",
    "x     = Dense(hidden_states, activation = \"relu\")(input)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "x     = Dense(hidden_states, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "x     = Dense(hidden_states, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "x     = Dense(hidden_states, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "out   = Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs = input, outputs = out)\n",
    "\n",
    "model.compile(\n",
    "    loss    = BinaryCrossentropy(from_logits = False),\n",
    "    metrics   = [BinaryAccuracy(name = \"Acc\"), AUC(name = \"AUC\", from_logits = True)],\n",
    "    optimizer = Adam(learning_rate = learning_rate)\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "if test_lr:\n",
    "    finder = LRFinder(model)\n",
    "    finder.find(X_train_scaled, y_train, test_interval[0], test_interval[1], batch_size = batch_size)\n",
    "    finder.plot_loss()\n",
    "\n",
    "if fit_model:\n",
    "    model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs = epochs,\n",
    "        batch_size = batch_size,\n",
    "        validation_split = val_split,\n",
    "        callbacks = EarlyStopping(\"val_loss\", patience = patience, restore_best_weights = True)\n",
    "        )\n",
    "\n",
    "if test_model:\n",
    "    model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "    test_preds = model(X_test_scaled)\n",
    "\n",
    "    if save_preds:\n",
    "        with open(f\"../predictions/{name}.dat\", \"wb\") as file:\n",
    "            pickle.dump((test_preds, y_test), file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D-conv:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_36\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_51 (InputLayer)       [(None, 220)]             0         \n",
      "                                                                 \n",
      " tf.expand_dims_9 (TFOpLambd  (None, 220, 1)           0         \n",
      " a)                                                              \n",
      "                                                                 \n",
      " conv1d_89 (Conv1D)          (None, 120, 32)           3264      \n",
      "                                                                 \n",
      " conv1d_90 (Conv1D)          (None, 70, 16)            26128     \n",
      "                                                                 \n",
      " conv1d_91 (Conv1D)          (None, 46, 16)            6416      \n",
      "                                                                 \n",
      " conv1d_92 (Conv1D)          (None, 37, 8)             1288      \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 296)               0         \n",
      "                                                                 \n",
      " dense_142 (Dense)           (None, 32)                9504      \n",
      "                                                                 \n",
      " dropout_117 (Dropout)       (None, 32)                0         \n",
      "                                                                 \n",
      " dense_143 (Dense)           (None, 8)                 264       \n",
      "                                                                 \n",
      " dropout_118 (Dropout)       (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_144 (Dense)           (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,873\n",
      "Trainable params: 46,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "213/213 [==============================] - 13s 59ms/step - loss: 0.5951 - Acc: 0.7325 - AUC: 0.7802 - val_loss: 0.5042 - val_Acc: 0.8204 - val_AUC: 0.8541\n",
      "Epoch 2/50\n",
      "213/213 [==============================] - 12s 58ms/step - loss: 0.4998 - Acc: 0.7925 - AUC: 0.8530 - val_loss: 0.4553 - val_Acc: 0.8179 - val_AUC: 0.8904\n",
      "Epoch 3/50\n",
      "213/213 [==============================] - 11s 52ms/step - loss: 0.4690 - Acc: 0.8018 - AUC: 0.8700 - val_loss: 0.4137 - val_Acc: 0.8458 - val_AUC: 0.8978\n",
      "Epoch 4/50\n",
      "213/213 [==============================] - 11s 51ms/step - loss: 0.4523 - Acc: 0.8156 - AUC: 0.8798 - val_loss: 0.4022 - val_Acc: 0.8479 - val_AUC: 0.8989\n",
      "Epoch 5/50\n",
      "213/213 [==============================] - 13s 59ms/step - loss: 0.4451 - Acc: 0.8163 - AUC: 0.8828 - val_loss: 0.4029 - val_Acc: 0.8496 - val_AUC: 0.9023\n",
      "Epoch 6/50\n",
      "213/213 [==============================] - 19s 87ms/step - loss: 0.4492 - Acc: 0.8184 - AUC: 0.8818 - val_loss: 0.3971 - val_Acc: 0.8487 - val_AUC: 0.9040\n",
      "Epoch 7/50\n",
      "213/213 [==============================] - 15s 70ms/step - loss: 0.4410 - Acc: 0.8203 - AUC: 0.8862 - val_loss: 0.3981 - val_Acc: 0.8400 - val_AUC: 0.9037\n",
      "Epoch 8/50\n",
      "213/213 [==============================] - 29s 139ms/step - loss: 0.4340 - Acc: 0.8263 - AUC: 0.8877 - val_loss: 0.3945 - val_Acc: 0.8471 - val_AUC: 0.9074\n",
      "Epoch 9/50\n",
      "213/213 [==============================] - 14s 67ms/step - loss: 0.4235 - Acc: 0.8304 - AUC: 0.8939 - val_loss: 0.4024 - val_Acc: 0.8392 - val_AUC: 0.9016\n",
      "Epoch 10/50\n",
      "213/213 [==============================] - 12s 55ms/step - loss: 0.4288 - Acc: 0.8292 - AUC: 0.8921 - val_loss: 0.4044 - val_Acc: 0.8467 - val_AUC: 0.8996\n",
      "Epoch 11/50\n",
      "213/213 [==============================] - 12s 56ms/step - loss: 0.4250 - Acc: 0.8318 - AUC: 0.8935 - val_loss: 0.3980 - val_Acc: 0.8483 - val_AUC: 0.9039\n",
      "Epoch 12/50\n",
      "213/213 [==============================] - 11s 50ms/step - loss: 0.4147 - Acc: 0.8370 - AUC: 0.8976 - val_loss: 0.4025 - val_Acc: 0.8471 - val_AUC: 0.9028\n",
      "Epoch 13/50\n",
      "213/213 [==============================] - 10s 46ms/step - loss: 0.4148 - Acc: 0.8333 - AUC: 0.8979 - val_loss: 0.3997 - val_Acc: 0.8492 - val_AUC: 0.9005\n",
      "Epoch 14/50\n",
      "213/213 [==============================] - 11s 54ms/step - loss: 0.4064 - Acc: 0.8401 - AUC: 0.9027 - val_loss: 0.3970 - val_Acc: 0.8413 - val_AUC: 0.9022\n",
      "Epoch 15/50\n",
      "213/213 [==============================] - 12s 58ms/step - loss: 0.4093 - Acc: 0.8377 - AUC: 0.9010 - val_loss: 0.3916 - val_Acc: 0.8521 - val_AUC: 0.9012\n",
      "Epoch 16/50\n",
      "213/213 [==============================] - 11s 54ms/step - loss: 0.4027 - Acc: 0.8432 - AUC: 0.9038 - val_loss: 0.3977 - val_Acc: 0.8492 - val_AUC: 0.9019\n",
      "Epoch 17/50\n",
      "213/213 [==============================] - 15s 70ms/step - loss: 0.4115 - Acc: 0.8378 - AUC: 0.8984 - val_loss: 0.3991 - val_Acc: 0.8504 - val_AUC: 0.9019\n",
      "Epoch 18/50\n",
      "213/213 [==============================] - 30s 143ms/step - loss: 0.3988 - Acc: 0.8417 - AUC: 0.9051 - val_loss: 0.4068 - val_Acc: 0.8471 - val_AUC: 0.8986\n",
      "Epoch 19/50\n",
      "213/213 [==============================] - 44s 208ms/step - loss: 0.4021 - Acc: 0.8411 - AUC: 0.9033 - val_loss: 0.4045 - val_Acc: 0.8458 - val_AUC: 0.9049\n",
      "Epoch 20/50\n",
      "213/213 [==============================] - 37s 176ms/step - loss: 0.3901 - Acc: 0.8476 - AUC: 0.9093 - val_loss: 0.4076 - val_Acc: 0.8462 - val_AUC: 0.9007\n",
      "Epoch 21/50\n",
      "213/213 [==============================] - 40s 187ms/step - loss: 0.3851 - Acc: 0.8500 - AUC: 0.9100 - val_loss: 0.4120 - val_Acc: 0.8450 - val_AUC: 0.8999\n",
      "Epoch 22/50\n",
      "213/213 [==============================] - 39s 185ms/step - loss: 0.3890 - Acc: 0.8459 - AUC: 0.9090 - val_loss: 0.4111 - val_Acc: 0.8379 - val_AUC: 0.8990\n",
      "Epoch 23/50\n",
      "213/213 [==============================] - 17s 81ms/step - loss: 0.3807 - Acc: 0.8521 - AUC: 0.9129 - val_loss: 0.4083 - val_Acc: 0.8404 - val_AUC: 0.9000\n",
      "Epoch 24/50\n",
      "213/213 [==============================] - 6s 28ms/step - loss: 0.3764 - Acc: 0.8538 - AUC: 0.9141 - val_loss: 0.4254 - val_Acc: 0.8317 - val_AUC: 0.8941\n",
      "Epoch 25/50\n",
      "213/213 [==============================] - 6s 29ms/step - loss: 0.3763 - Acc: 0.8546 - AUC: 0.9145 - val_loss: 0.4133 - val_Acc: 0.8483 - val_AUC: 0.8953\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 0.3814 - Acc: 0.8575 - AUC: 0.9040\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "name           = \"1D_conv\"\n",
    "dropout        = 0.5\n",
    "filter_sizes   = (32,  16, 16, 8)\n",
    "kernal_sizes   = (101, 51, 25, 10)\n",
    "hidden_states  = 32 \n",
    "learning_rate  = 1e-3\n",
    "test_lr        = False\n",
    "test_interval  = (1e-9, 1)\n",
    "fit_model      = True\n",
    "test_model     = True\n",
    "save_preds     = True\n",
    "\n",
    "input = Input(shape = (timesteps))\n",
    "\n",
    "x = input\n",
    "x = tf.expand_dims(x, -1)\n",
    "\n",
    "for fs, ks in zip(filter_sizes, kernal_sizes):\n",
    "    x     = Conv1D(fs, kernel_size = ks, activation = \"relu\")(x)\n",
    "\n",
    "x     = Flatten()(x)\n",
    "\n",
    "# x     = tf.squeeze(x, -1)\n",
    "\n",
    "x     = Dense(hidden_states, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "x     = Dense(hidden_states // 4, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "out   = Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs = input, outputs = out)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss    = BinaryCrossentropy(from_logits = False),\n",
    "    metrics   = [BinaryAccuracy(name = \"Acc\"), AUC(name = \"AUC\", from_logits = True)],\n",
    "    optimizer = Adam(learning_rate = learning_rate)\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "if test_lr:\n",
    "    finder = LRFinder(model)\n",
    "    finder.find(X_train_scaled, y_train, test_interval[0], test_interval[1], batch_size = batch_size)\n",
    "    finder.plot_loss()\n",
    "\n",
    "if fit_model:\n",
    "    model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs = epochs,\n",
    "        batch_size = batch_size,\n",
    "        validation_split = val_split,\n",
    "        callbacks = EarlyStopping(\"val_loss\", patience = patience, restore_best_weights = True)\n",
    "        )\n",
    "\n",
    "if test_model:\n",
    "    model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "    test_preds = model(X_test_scaled)\n",
    "\n",
    "    if save_preds:\n",
    "        with open(f\"../predictions/{name}.dat\", \"wb\") as file:\n",
    "            pickle.dump((test_preds, y_test), file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_40\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_55 (InputLayer)       [(None, 220)]             0         \n",
      "                                                                 \n",
      " tf.expand_dims_13 (TFOpLamb  (None, 220, 1)           0         \n",
      " da)                                                             \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 220, 4)            96        \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 220, 8)            416       \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 220, 4)            208       \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 880)               0         \n",
      "                                                                 \n",
      " dense_154 (Dense)           (None, 32)                28192     \n",
      "                                                                 \n",
      " dropout_125 (Dropout)       (None, 32)                0         \n",
      "                                                                 \n",
      " dense_155 (Dense)           (None, 8)                 264       \n",
      "                                                                 \n",
      " dropout_126 (Dropout)       (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_156 (Dense)           (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,185\n",
      "Trainable params: 29,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "213/213 [==============================] - 213s 971ms/step - loss: 0.4810 - Acc: 0.7940 - AUC: 0.8527 - val_loss: 0.3939 - val_Acc: 0.8429 - val_AUC: 0.8996\n",
      "Epoch 2/50\n",
      "213/213 [==============================] - 146s 683ms/step - loss: 0.4194 - Acc: 0.8324 - AUC: 0.8879 - val_loss: 0.3809 - val_Acc: 0.8508 - val_AUC: 0.9071\n",
      "Epoch 3/50\n",
      "213/213 [==============================] - 82s 386ms/step - loss: 0.4111 - Acc: 0.8410 - AUC: 0.8912 - val_loss: 0.3821 - val_Acc: 0.8475 - val_AUC: 0.9069\n",
      "Epoch 4/50\n",
      "213/213 [==============================] - 91s 427ms/step - loss: 0.4067 - Acc: 0.8423 - AUC: 0.8955 - val_loss: 0.3791 - val_Acc: 0.8467 - val_AUC: 0.9068\n",
      "Epoch 5/50\n",
      "213/213 [==============================] - 101s 475ms/step - loss: 0.3986 - Acc: 0.8464 - AUC: 0.8975 - val_loss: 0.3812 - val_Acc: 0.8533 - val_AUC: 0.9076\n",
      "Epoch 6/50\n",
      "213/213 [==============================] - 106s 497ms/step - loss: 0.3929 - Acc: 0.8474 - AUC: 0.9001 - val_loss: 0.4270 - val_Acc: 0.8217 - val_AUC: 0.8829\n",
      "Epoch 7/50\n",
      "213/213 [==============================] - 96s 452ms/step - loss: 0.3978 - Acc: 0.8466 - AUC: 0.8969 - val_loss: 0.3701 - val_Acc: 0.8567 - val_AUC: 0.9101\n",
      "Epoch 8/50\n",
      "213/213 [==============================] - 104s 487ms/step - loss: 0.3884 - Acc: 0.8491 - AUC: 0.9023 - val_loss: 0.3674 - val_Acc: 0.8567 - val_AUC: 0.9139\n",
      "Epoch 9/50\n",
      "213/213 [==============================] - 110s 519ms/step - loss: 0.3906 - Acc: 0.8487 - AUC: 0.9017 - val_loss: 0.3698 - val_Acc: 0.8558 - val_AUC: 0.9137\n",
      "Epoch 10/50\n",
      "213/213 [==============================] - 127s 598ms/step - loss: 0.3860 - Acc: 0.8521 - AUC: 0.9032 - val_loss: 0.3685 - val_Acc: 0.8571 - val_AUC: 0.9107\n",
      "Epoch 11/50\n",
      "213/213 [==============================] - 123s 576ms/step - loss: 0.3866 - Acc: 0.8526 - AUC: 0.9035 - val_loss: 0.3903 - val_Acc: 0.8537 - val_AUC: 0.9113\n",
      "Epoch 12/50\n",
      "213/213 [==============================] - 106s 496ms/step - loss: 0.3892 - Acc: 0.8526 - AUC: 0.9025 - val_loss: 0.3629 - val_Acc: 0.8575 - val_AUC: 0.9145\n",
      "Epoch 13/50\n",
      "213/213 [==============================] - 108s 509ms/step - loss: 0.3820 - Acc: 0.8532 - AUC: 0.9058 - val_loss: 0.3591 - val_Acc: 0.8604 - val_AUC: 0.9171\n",
      "Epoch 14/50\n",
      "213/213 [==============================] - 116s 546ms/step - loss: 0.3827 - Acc: 0.8541 - AUC: 0.9060 - val_loss: 0.3831 - val_Acc: 0.8512 - val_AUC: 0.9043\n",
      "Epoch 15/50\n",
      "213/213 [==============================] - 113s 529ms/step - loss: 0.3761 - Acc: 0.8568 - AUC: 0.9093 - val_loss: 0.3645 - val_Acc: 0.8558 - val_AUC: 0.9145\n",
      "Epoch 16/50\n",
      "213/213 [==============================] - 105s 494ms/step - loss: 0.3764 - Acc: 0.8573 - AUC: 0.9079 - val_loss: 0.3619 - val_Acc: 0.8604 - val_AUC: 0.9132\n",
      "Epoch 17/50\n",
      "213/213 [==============================] - 204s 960ms/step - loss: 0.3755 - Acc: 0.8538 - AUC: 0.9101 - val_loss: 0.3667 - val_Acc: 0.8587 - val_AUC: 0.9116\n",
      "Epoch 18/50\n",
      "213/213 [==============================] - 254s 1s/step - loss: 0.3690 - Acc: 0.8577 - AUC: 0.9137 - val_loss: 0.3899 - val_Acc: 0.8537 - val_AUC: 0.9049\n",
      "Epoch 19/50\n",
      "213/213 [==============================] - 199s 937ms/step - loss: 0.3660 - Acc: 0.8583 - AUC: 0.9144 - val_loss: 0.3615 - val_Acc: 0.8612 - val_AUC: 0.9163\n",
      "Epoch 20/50\n",
      "213/213 [==============================] - 109s 512ms/step - loss: 0.3656 - Acc: 0.8598 - AUC: 0.9151 - val_loss: 0.3694 - val_Acc: 0.8617 - val_AUC: 0.9104\n",
      "Epoch 21/50\n",
      "213/213 [==============================] - 175s 811ms/step - loss: 0.3656 - Acc: 0.8602 - AUC: 0.9152 - val_loss: 0.3683 - val_Acc: 0.8550 - val_AUC: 0.9112\n",
      "Epoch 22/50\n",
      "213/213 [==============================] - 90s 421ms/step - loss: 0.3603 - Acc: 0.8598 - AUC: 0.9181 - val_loss: 0.3854 - val_Acc: 0.8446 - val_AUC: 0.9063\n",
      "Epoch 23/50\n",
      "213/213 [==============================] - 111s 522ms/step - loss: 0.3597 - Acc: 0.8612 - AUC: 0.9183 - val_loss: 0.3747 - val_Acc: 0.8596 - val_AUC: 0.9134\n",
      "125/125 [==============================] - 33s 245ms/step - loss: 0.3669 - Acc: 0.8585 - AUC: 0.9121\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "name           = \"LSTM\"\n",
    "dropout        = 0.15\n",
    "hidden_states  = 32 \n",
    "units          = (4, 8, 4)\n",
    "learning_rate  = 1e-2\n",
    "test_lr        = False\n",
    "test_interval  = (1e-9, 1)\n",
    "fit_model      = True\n",
    "test_model     = True\n",
    "save_preds     = True\n",
    "\n",
    "input = Input(shape = (timesteps))\n",
    "\n",
    "x = input\n",
    "x = tf.expand_dims(input, -1)\n",
    "\n",
    "for u in units:\n",
    "    x     = LSTM(u, dropout = dropout, return_sequences = True)(x)\n",
    "\n",
    "x     = Flatten()(x)\n",
    "\n",
    "x     = Dense(hidden_states, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "x     = Dense(hidden_states // 4, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "out   = Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs = input, outputs = out)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss    = BinaryCrossentropy(from_logits = False),\n",
    "    metrics   = [BinaryAccuracy(name = \"Acc\"), AUC(name = \"AUC\", from_logits = True)],\n",
    "    optimizer = Adam(learning_rate = learning_rate)\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "if test_lr:\n",
    "    finder = LRFinder(model)\n",
    "    finder.find(X_train_scaled, y_train, test_interval[0], test_interval[1], batch_size = batch_size)\n",
    "    finder.plot_loss()\n",
    "\n",
    "if fit_model:\n",
    "    model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs = epochs,\n",
    "        batch_size = batch_size,\n",
    "        validation_split = val_split,\n",
    "        callbacks = EarlyStopping(\"val_loss\", patience = patience, restore_best_weights = True)\n",
    "        )\n",
    "\n",
    "if test_model:\n",
    "    model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "    test_preds = model(X_test_scaled)\n",
    "\n",
    "    if save_preds:\n",
    "        with open(f\"../predictions/{name}.dat\", \"wb\") as file:\n",
    "            pickle.dump((test_preds, y_test), file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tcn import TCN, tcn_full_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_53\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_68 (InputLayer)       [(None, 220)]             0         \n",
      "                                                                 \n",
      " tf.expand_dims_26 (TFOpLamb  (None, 220, 1)           0         \n",
      " da)                                                             \n",
      "                                                                 \n",
      " tcn_16 (TCN)                (None, 220, 8)            2248      \n",
      "                                                                 \n",
      " flatten_18 (Flatten)        (None, 1760)              0         \n",
      "                                                                 \n",
      " dense_193 (Dense)           (None, 32)                56352     \n",
      "                                                                 \n",
      " dropout_151 (Dropout)       (None, 32)                0         \n",
      "                                                                 \n",
      " dense_194 (Dense)           (None, 8)                 264       \n",
      "                                                                 \n",
      " dropout_152 (Dropout)       (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_195 (Dense)           (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 58,873\n",
      "Trainable params: 58,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "213/213 [==============================] - 85s 318ms/step - loss: 1.1507 - Acc: 0.6089 - AUC: 0.6445 - val_loss: 0.6335 - val_Acc: 0.7150 - val_AUC: 0.7485\n",
      "Epoch 2/50\n",
      "213/213 [==============================] - 53s 249ms/step - loss: 0.6502 - Acc: 0.6817 - AUC: 0.7220 - val_loss: 0.6145 - val_Acc: 0.7208 - val_AUC: 0.8048\n",
      "Epoch 3/50\n",
      "213/213 [==============================] - 51s 238ms/step - loss: 0.6129 - Acc: 0.7140 - AUC: 0.7430 - val_loss: 0.5896 - val_Acc: 0.7433 - val_AUC: 0.8167\n",
      "Epoch 4/50\n",
      "213/213 [==============================] - 55s 256ms/step - loss: 0.5938 - Acc: 0.7273 - AUC: 0.7570 - val_loss: 0.5918 - val_Acc: 0.7208 - val_AUC: 0.8199\n",
      "Epoch 5/50\n",
      "213/213 [==============================] - 58s 272ms/step - loss: 0.5734 - Acc: 0.7424 - AUC: 0.7797 - val_loss: 0.5743 - val_Acc: 0.7267 - val_AUC: 0.8236\n",
      "Epoch 6/50\n",
      "213/213 [==============================] - 50s 235ms/step - loss: 0.5626 - Acc: 0.7523 - AUC: 0.7908 - val_loss: 0.5471 - val_Acc: 0.7571 - val_AUC: 0.8331\n",
      "Epoch 7/50\n",
      "213/213 [==============================] - 53s 250ms/step - loss: 0.5626 - Acc: 0.7530 - AUC: 0.7980 - val_loss: 0.5521 - val_Acc: 0.7429 - val_AUC: 0.8386\n",
      "Epoch 8/50\n",
      "213/213 [==============================] - 56s 263ms/step - loss: 0.5467 - Acc: 0.7584 - AUC: 0.8064 - val_loss: 0.5617 - val_Acc: 0.7237 - val_AUC: 0.8405\n",
      "Epoch 9/50\n",
      "213/213 [==============================] - 51s 240ms/step - loss: 0.5386 - Acc: 0.7626 - AUC: 0.8201 - val_loss: 0.5280 - val_Acc: 0.7592 - val_AUC: 0.8550\n",
      "Epoch 10/50\n",
      "213/213 [==============================] - 48s 227ms/step - loss: 0.5376 - Acc: 0.7671 - AUC: 0.8311 - val_loss: 0.5350 - val_Acc: 0.7371 - val_AUC: 0.8625\n",
      "Epoch 11/50\n",
      "213/213 [==============================] - 54s 256ms/step - loss: 0.5236 - Acc: 0.7713 - AUC: 0.8437 - val_loss: 0.5089 - val_Acc: 0.7558 - val_AUC: 0.8732\n",
      "Epoch 12/50\n",
      "213/213 [==============================] - 52s 243ms/step - loss: 0.5077 - Acc: 0.7796 - AUC: 0.8508 - val_loss: 0.5093 - val_Acc: 0.7571 - val_AUC: 0.8742\n",
      "Epoch 13/50\n",
      "213/213 [==============================] - 46s 217ms/step - loss: 0.5045 - Acc: 0.7787 - AUC: 0.8519 - val_loss: 0.5094 - val_Acc: 0.7613 - val_AUC: 0.8721\n",
      "Epoch 14/50\n",
      "213/213 [==============================] - 47s 220ms/step - loss: 0.4953 - Acc: 0.7865 - AUC: 0.8592 - val_loss: 0.4841 - val_Acc: 0.7917 - val_AUC: 0.8746\n",
      "Epoch 15/50\n",
      "213/213 [==============================] - 49s 231ms/step - loss: 0.4895 - Acc: 0.7922 - AUC: 0.8603 - val_loss: 0.4882 - val_Acc: 0.7842 - val_AUC: 0.8817\n",
      "Epoch 16/50\n",
      "213/213 [==============================] - 42s 195ms/step - loss: 0.4864 - Acc: 0.7997 - AUC: 0.8612 - val_loss: 0.4782 - val_Acc: 0.7954 - val_AUC: 0.8817\n",
      "Epoch 17/50\n",
      "213/213 [==============================] - 42s 196ms/step - loss: 0.4837 - Acc: 0.8007 - AUC: 0.8652 - val_loss: 0.4639 - val_Acc: 0.8171 - val_AUC: 0.8815\n",
      "Epoch 18/50\n",
      "213/213 [==============================] - 33s 156ms/step - loss: 0.4818 - Acc: 0.8032 - AUC: 0.8657 - val_loss: 0.4606 - val_Acc: 0.8179 - val_AUC: 0.8836\n",
      "Epoch 19/50\n",
      "213/213 [==============================] - 12s 55ms/step - loss: 0.4775 - Acc: 0.8021 - AUC: 0.8662 - val_loss: 0.4690 - val_Acc: 0.8087 - val_AUC: 0.8843\n",
      "Epoch 20/50\n",
      "213/213 [==============================] - 14s 64ms/step - loss: 0.4742 - Acc: 0.8052 - AUC: 0.8669 - val_loss: 0.4610 - val_Acc: 0.8121 - val_AUC: 0.8851\n",
      "Epoch 21/50\n",
      "213/213 [==============================] - 14s 67ms/step - loss: 0.4704 - Acc: 0.8107 - AUC: 0.8682 - val_loss: 0.4649 - val_Acc: 0.8100 - val_AUC: 0.8853\n",
      "125/125 [==============================] - 3s 15ms/step - loss: 0.4553 - Acc: 0.8077 - AUC: 0.8893\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "name           = \"TCN\"\n",
    "dropout        = 0.25\n",
    "hidden_states  = 32\n",
    "nb_filters     = 8\n",
    "kernel_size    = 3\n",
    "dilations      = (1, 2, 4, 8, 16, 32) \n",
    "learning_rate  = 1e-3\n",
    "test_lr        = False\n",
    "test_interval  = (1e-9, 1)\n",
    "fit_model      = True\n",
    "test_model     = True\n",
    "save_preds     = True\n",
    "\n",
    "input = Input(shape = (timesteps))\n",
    "\n",
    "x     = input\n",
    "x     = tf.expand_dims(input, -1)\n",
    "\n",
    "x     = TCN(\n",
    "    nb_filters  = nb_filters,\n",
    "    kernel_size = kernel_size,\n",
    "    dilations   = dilations,\n",
    "    dropout_rate = dropout,\n",
    "    return_sequences = True)(x)\n",
    "\n",
    "x     = Flatten()(x)\n",
    "\n",
    "x     = Dense(hidden_states, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "x     = Dense(hidden_states // 4, activation = \"relu\")(x)\n",
    "x     = Dropout(dropout)(x)\n",
    "\n",
    "out   = Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs = input, outputs = out)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss    = BinaryCrossentropy(from_logits = False),\n",
    "    metrics   = [BinaryAccuracy(name = \"Acc\"), AUC(name = \"AUC\", from_logits = True)],\n",
    "    optimizer = Adam(learning_rate = learning_rate)\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "if test_lr:\n",
    "    finder = LRFinder(model)\n",
    "    finder.find(X_train_scaled, y_train, test_interval[0], test_interval[1], batch_size = batch_size)\n",
    "    finder.plot_loss()\n",
    "\n",
    "if fit_model:\n",
    "    model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs = epochs,\n",
    "        batch_size = batch_size,\n",
    "        validation_split = val_split,\n",
    "        callbacks = EarlyStopping(\"val_loss\", patience = patience, restore_best_weights = True)\n",
    "        )\n",
    "\n",
    "if test_model:\n",
    "    model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "    test_preds = model(X_test_scaled)\n",
    "\n",
    "    if save_preds:\n",
    "        with open(f\"../predictions/{name}.dat\", \"wb\") as file:\n",
    "            pickle.dump((test_preds, y_test), file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "609b8e94a29404947100e7ebc1571deda4e76c5e365f7a0289290abec32388f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
